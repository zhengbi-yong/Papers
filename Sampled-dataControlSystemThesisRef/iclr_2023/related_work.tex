\textbf{Robust transformer.}
Ensuring the generalization and robustness of both vision transformer and language model remains an ongoing research focus. Large language models are vulnerable to input corruption~\cite{005357537ef34c158137a70c5df3799b, peyrard-etal-2022-invariant, jin2020bert, zang2019word}, posing a challenge in developing robust real-world applications that can withstand unforeseen adversarial threats. For ViTs, investigations into model robustness against adversarial attacks, domain shifts, and out-of-distribution data are crucial for real-world deployment. Techniques such as data augmentation, regularization, and adversarial training are actively explored to enhance the robustness and generalization capabilities of ViTs. Many investigations (e.g.,~\cite{yuan2023you, paul2022vision, mahmood2021robustness,
bhojanapalli2021understanding, madry2017towards, mao2022towards, zhou2022understanding}) have attempted to explain and improve the resilience of ViT models against typical adversarial attacks. For example,~\cite{mahmood2021robustness} empirically mitigates ViT's vulnerability to white-box adversarial attacks by introducing a simple ensemble defense strategy that notably enhanced robustness without sacrificing accuracy on clean data.
% Moreover,~\cite{mao2022towards} performed a thorough examination of the robustness of different components of ViT models, suggesting techniques such as position-aware attention scaling and patch-wise augmentation to improve the model's resilience.

\textbf{Rank-collapse in transformer.} Rank collapse in deep transformers, observed across domains from natural language processing~\cite{shi2022revisiting} to computer vision~\cite{wang2022antioversmoothing,dong2021attention}, is evident. In computer vision, \citeauthor{zhou2021deepvit} (\citeyear{zhou2021deepvit}) find that adding more layers to the Vision Transformer (ViT)~\cite{DosovitskiyB0WZ21} quickly saturates its performance. Moreover, their experiments show that a 32-layer ViT performs worse than a 24-layer ViT, attributed to token representations becoming identical with increasing model depth. To address this matter, \cite{wang2022antioversmoothing} discovers that self-attention functions as a low-pass filter, causing token representations in ViTs to be smoothed.
% Consequently, the FeatScale technique is introduced \cite{wang2022antioversmoothing}, which adjusts feature frequencies, be they low or high, to counteract the effects of excessive smoothing. 
Furthermore, \cite{shi2022revisiting} identifies a similar phenomenon in BERT~\cite{devlin2018bert}, and investigates rank-collapse from a graph perspective. The study employs hierarchical fusion techniques to retain the output of self-attention across all layers.
Our work is orthogonal to the existing method as we develop a control framework to tackle the non-robustness and rank-collapse issues in transformers. 
% Leveraging this framework, we propose the Proportional-Integral-Derivative (PID) transformer (PIDformer), which is proven to enhance robustness and mitigate rank collapse in token representations.
