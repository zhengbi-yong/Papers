
@inproceedings{
shi2022revisiting,
title={Revisiting Over-smoothing in {BERT} from the Perspective of Graph},
author={Han Shi and JIAHUI GAO and Hang Xu and Xiaodan Liang and Zhenguo Li and Lingpeng Kong and Stephen M. S. Lee and James Kwok},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=dUV91uaXm3}
}

@inproceedings{wang2022antioversmoothing,
title={Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice},
author={Wang, Peihao and Zheng, Wenqing and Chen, Tianlong and Wang, Zhangyang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=O476oWmiNNp},
}

@article{chang2008perron,
  title={Perron-Frobenius theorem for nonnegative tensors},
  author={Chang, Kung-Ching and Pearson, Kelly and Zhang, Tan},
  journal={Communications in Mathematical Sciences},
  volume={6},
  number={2},
  pages={507--520},
  year={2008},
  publisher={International Press of Boston}
}

@book{strohmer2020mathdl,
  added-at = {2020-06-17T18:39:03.000+0200},
  author = {Bandeira, Alfonso S. and Singer, Amit and Strohmer, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2c938436eaf49f4fc3e4a37d9e0184a83/kirk86},
  description = {BandeiraSingerStrohmer-MDS-draft.pdf},
  interhash = {d7446f64b564e488a2f3c89424fea6f3},
  intrahash = {c938436eaf49f4fc3e4a37d9e0184a83},
  keywords = {book machine-learning mathematics readings},
  timestamp = {2020-06-17T18:44:25.000+0200},
  title = {Mathematics of Data Science},
  url = {https://people.math.ethz.ch/~abandeira/BandeiraSingerStrohmer-MDS-draft.pdf},
  year = 2020
}

@book{euler1792institutiones,
  title={Institutiones calculi integralis},
  author={Euler, Leonhard},
  volume={1},
  year={1792},
  publisher={impensis Academiae imperialis scientiarum}
}

@article{Kindermann2005DeblurringAD,
  title={Deblurring and Denoising of Images by Nonlocal Functionals},
  author={Stefan Kindermann and S. Osher and Peter W. Jones},
  journal={Multiscale Model. Simul.},
  year={2005},
  volume={4},
  pages={1091-1115}
}
@inproceedings{DBLP:conf/iclr/MerityX0S17,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Byj72udxe},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Thorpe2022GRANDGN,
  title={GRAND++: Graph Neural Diffusion with A Source Term},
  author={Matthew Thorpe and Tan Minh Nguyen and Hedi Xia and Thomas Strohmer and A. Bertozzi and Stanley J. Osher and Bao Wang},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@article{Gilboa2008NonlocalOW,
  title={Nonlocal Operators with Applications to Image Processing},
  author={Guy Gilboa and S. Osher},
  journal={Multiscale Model. Simul.},
  year={2008},
  volume={7},
  pages={1005-1028}
}
@article{Gilboa2007NonlocalLI,
  title={Nonlocal Linear Image Regularization and Supervised Segmentation},
  author={Guy Gilboa and S. Osher},
  journal={Multiscale Model. Simul.},
  year={2007},
  volume={6},
  pages={595-630}
}
@misc{zhou2018semantic,
      title={Semantic Understanding of Scenes through the ADE20K Dataset}, 
      author={Bolei Zhou and Hang Zhao and Xavier Puig and Tete Xiao and Sanja Fidler and Adela Barriuso and Antonio Torralba},
      year={2018},
      eprint={1608.05442},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}
@inproceedings{agirre-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1051",
    pages = "385--393",
}
@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}
@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15262--15271},
  year={2021}
}
@inproceedings{DosovitskiyB0WZ21,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gong2021vision,
  title={Vision transformers with patch diversification},
  author={Gong, Chengyue and Wang, Dilin and Li, Meng and Chandra, Vikas and Liu, Qiang},
  journal={arXiv preprint arXiv:2104.12753},
  year={2021}
}
@InProceedings{Touvron_2021_ICCV,
    author    = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J\'egou, Herv\'e},
    title     = {Going Deeper With Image Transformers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {32-42}
}

@misc{zhou2021deepvit,
      title={DeepViT: Towards Deeper Vision Transformer}, 
      author={Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Zihang Jiang and Qibin Hou and Jiashi Feng},
      year={2021},
      eprint={2103.11886},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}

@article{RUDIN1992259,
title = {Nonlinear total variation based noise removal algorithms},
journal = {Physica D: Nonlinear Phenomena},
volume = {60},
number = {1},
pages = {259-268},
year = {1992},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(92)90242-F},
url = {https://www.sciencedirect.com/science/article/pii/016727899290242F},
author = {Leonid I. Rudin and Stanley Osher and Emad Fatemi},
abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lanrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t → ∞ the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.}
}
@techreport{ghahramani1996algorithm,
  title={The EM algorithm for mixtures of factor analyzers},
  author={Ghahramani, Zoubin and Hinton, Geoffrey E and others},
  year={1996},
  institution={Technical Report CRG-TR-96-1, University of Toronto}
}

@article{li2021differentiable,
  title={Differentiable subset pruning of transformer heads},
  author={Li, Jiaoda and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1442--1459},
  year={2021},
  publisher={MIT Press}
}

@article{Osher2005AnIR,
  title={An Iterative Regularization Method for Total Variation-Based Image Restoration},
  author={S. Osher and Martin Burger and Donald Goldfarb and Jinjun Xu and Wotao Yin},
  journal={Multiscale Model. Simul.},
  year={2005},
  volume={4},
  pages={460-489}
}

@inproceedings{zhou2017scene,
  title={Scene parsing through ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={633--641},
  year={2017}
}

@article{kim2021learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2107.00910},
  year={2021}
}
@article{doi:10.1137/040616024,
author = {Buades, A. and Coll, B. and Morel, J. M.},
title = {A Review of Image Denoising Algorithms, with a New One},
journal = {Multiscale Modeling \& Simulation},
volume = {4},
number = {2},
pages = {490-530},
year = {2005},
doi = {10.1137/040616024},

URL = { 
    
        https://doi.org/10.1137/040616024
    
    

},
eprint = { 
    
        https://doi.org/10.1137/040616024
    
    

}
,
    abstract = { The search for efficient image denoising methods is still a valid challenge at the crossing of functional analysis and statistics. In spite of the sophistication of the recently proposed methods, most algorithms have not yet attained a desirable level of applicability. All show an outstanding performance when the image model corresponds to the algorithm assumptions but fail in general and create artifacts or remove image fine structures. The main focus of this paper is, first, to define a general mathematical and experimental methodology to compare and classify classical image denoising algorithms and, second, to propose a nonlocal means (NL-means) algorithm addressing the preservation of structure in a digital image. The mathematical analysis is based on the analysis of the "method noise," defined as the difference between a digital image and its denoised version. The NL-means algorithm is proven to be asymptotically optimal under a generic statistical image model. The denoising performance of all considered methods are compared in four ways; mathematical: asymptotic order of magnitude of the method noise under regularity assumptions; perceptual-mathematical: the algorithms artifacts and their explanation as a violation of the image model; quantitative experimental: by tables of L2 distances of the denoised version to the original image. The most powerful evaluation method seems, however, to be the visualization of the method noise on natural images. The more this method noise looks like a real white noise, the better the method. }
}

@article{anisotropic,
author = {Weickert, Joachim and Werdegang, Wissenschaftlicher and Zucker, Steven and Dobbins, Allan and Iverson, Lee and Kimia, Benjamin and Tannenbaum, Allen},
year = {1996},
month = {01},
pages = {},
title = {Anisotropic Diffusion In Image Processing}
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7262--7272},
  year={2021}
}

@inproceedings{kim-cho-2021-length,
    title = "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",
    author = "Kim, Gyuwan  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.508",
    doi = "10.18653/v1/2021.acl-long.508",
    pages = "6501--6511",
    abstract = "Despite transformers{'} impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer.",
}
@inproceedings{simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
    abstract = "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
}
@inproceedings{goyal2020power,
  title={PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination},
  author={Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
  booktitle={International Conference on Machine Learning},
  pages={3690--3699},
  year={2020},
  organization={PMLR}
}

@article{Kotzias2015FromGT,
  title={From Group to Individual Labels Using Deep Features},
  author={Dimitrios Kotzias and Misha Denil and Nando de Freitas and Padhraic Smyth},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:7729996}
}
@inproceedings{kahardipraja-etal-2021-towards,
    title = "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}",
    author = "Kahardipraja, Patrick  and
      Madureira, Brielen  and
      Schlangen, David",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.90",
    doi = "10.18653/v1/2021.emnlp-main.90",
    pages = "1178--1189",
    abstract = "Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.",
}
@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}


@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{nguyen2021transformer,
  title={Transformer with a Mixture of Gaussian Keys},
  author={Nguyen, Tam and Nguyen, Tan M and Le, Dung and Nguyen, Khuong and Tran, Anh and Baraniuk, Richard G and Ho, Nhat and Osher, Stanley J},
  journal={arXiv preprint arXiv:2110.08678},
  year={2021}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}


@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{schlag2021linear,
  title={Linear Transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@article{lloyd1982least,
  title={Least squares quantization in PCM},
  author={Lloyd, Stuart},
  journal={IEEE transactions on information theory},
  volume={28},
  number={2},
  pages={129--137},
  year={1982},
  publisher={IEEE}
}

@article{bishop2006pattern,
  title={Pattern recognition},
  author={Bishop, Christopher M},
  journal={Machine learning},
  volume={128},
  number={9},
  year={2006}
}

@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  year={2021},
  publisher={National Acad Sciences}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{wang2022transtab,
  title={{TransTab: Learning Transferable Tabular Transformers Across Tables}},
  author={Wang, Zifeng and Sun, Jimeng},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS 2022)},
  year={2022}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@article{zhang2019deep,
  title={Deep learning based recommender system: A survey and new perspectives},
  author={Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={1},
  pages={1--38},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@inproceedings{liu2021video,
	Author = {Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
	Booktitle = {I{EEE} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition (CVPR)},
	Title = {Video Swin Transformer},
	Year = {2022}}

@INPROCEEDINGS{9710415,
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={ViViT: A Video Vision Transformer}, 
  year={2021},
  volume={},
  number={},
  pages={6816-6826},
  doi={10.1109/ICCV48922.2021.00676}}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}


@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}

@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{smola2004tutorial,
  title={A tutorial on support vector regression},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  journal={Statistics and computing},
  volume={14},
  number={3},
  pages={199--222},
  year={2004},
  publisher={Springer}
}

@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@inproceedings{nangia-bowman-2018-listops,
    title = "{L}ist{O}ps: A Diagnostic Dataset for Latent Tree Learning",
    author = "Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-4013",
    doi = "10.18653/v1/N18-4013",
    pages = "92--99",
    abstract = "Latent tree learning models learn to parse a sentence without syntactic supervision, and use that parse to build the sentence representation. Existing work on such models has shown that, while they perform well on tasks like sentence classification, they do not learn grammars that conform to any plausible semantic or syntactic formalism (Williams et al., 2018a). Studying the parsing ability of such models in natural language can be challenging due to the inherent complexities of natural language, like having several valid parses for a single sentence. In this paper we introduce ListOps, a toy dataset created to study the parsing ability of latent tree models. ListOps sequences are in the style of prefix arithmetic. The dataset is designed to have a single correct parsing strategy that a system needs to learn to succeed at the task. We show that the current leading latent tree models are unable to learn to parse and succeed at ListOps. These models achieve accuracies worse than purely sequential RNNs.",
}

@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P11-1015",
    pages = "142--150",
}

@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  number={4},
  pages={919--944},
  year={2013},
  publisher={Springer}
}

@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{al2019character,
  title={Character-level language modeling with deeper self-attention},
  author={Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3159--3166},
  year={2019}
}

@inproceedings{MerityX0S17,
  author    = {Stephen Merity and
              Caiming Xiong and
              James Bradbury and
              Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
              Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Byj72udxe},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}










@INPROCEEDINGS{Papineni02bleu:a,
    author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-jing Zhu},
    title = {BLEU: a Method for Automatic Evaluation of Machine Translation},
    booktitle = {},
    year = {2002},
    pages = {311--318}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{sander2021momentum,
      title={Momentum Residual Neural Networks}, 
      author={Michael E. Sander and Pierre Ablin and Mathieu Blondel and Gabriel Peyré},
      year={2021},
      eprint={2102.07870},
      archivePrefix={arXiv},
      journal={arXiv preprint arXiv:2102.07870},
      primaryClass={cs.LG}
}

@inproceedings{li2018optimization,
  title={Optimization algorithm inspired deep neural network structure design},
  author={Li, Huan and Yang, Yibo and Chen, Dongmin and Lin, Zhouchen},
  booktitle={Asian Conference on Machine Learning},
  pages={614--629},
  year={2018},
  organization={PMLR}
}

@inproceedings{koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      et al.",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    url = "https://www.aclweb.org/anthology/P07-2045",
    pages = "177--180",
}

@inproceedings{lee-etal-2018-deterministic,
    title = "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement",
    author = "Lee, Jason  and
      Mansimov, Elman  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    url = "https://www.aclweb.org/anthology/D18-1149",
    doi = "10.18653/v1/D18-1149",
    pages = "1173--1182",
    abstract = "We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.",
}

@inproceedings{salimans2017pixelcnn,
title={Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications},
author={Tim Salimans and Andrej Karpathy and Xi Chen and Diederik Kingma},
booktitle={International Conference on Learning Representations},
year={2017}
}


@article{ramesh2020dalle,
  title={DALL·E: Creating Images from Text},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Chen, Mark and Child, Rewon and Misra, Vedant and Mishkin, Pamela and Krueger, Gretchen and Agarwal, Sandhini and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI report},
  year={2018}
}


@article{hochreiter1997long,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural Computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}
@article{mercer1909xvi,
  title={Xvi. functions of positive and negative type, and their connection the theory of integral equations},
  author={Mercer, James},
  journal={Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character},
  volume={209},
  number={441-458},
  pages={415--446},
  year={1909},
  publisher={The Royal Society London}
}

@article{kim2017structured,
  title={Structured attention networks},
  author={Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M},
  journal={arXiv preprint arXiv:1702.00887},
  year={2017}
}

@book{mordukhovich2006variational,
  title={Variational analysis and generalized differentiation I: Basic theory},
  author={Mordukhovich, Boris S},
  volume={330},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@inproceedings{huang2018music,
  title={Music transformer: Generating music with long-term structure},
  author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Simon, Ian and Hawthorne, Curtis and Shazeer, Noam and Dai, Andrew M and Hoffman, Matthew D and Dinculescu, Monica and Eck, Douglas},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{beck2009fast,
  title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  author={Beck, Amir and Teboulle, Marc},
  journal={SIAM Journal on Imaging Sciences},
  volume={2},
  number={1},
  pages={183--202},
  year={2009},
  publisher={SIAM}
}

@article{nesterov1998introductory,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  year={1998}
}

@book{rockafellar2009variational,
  title={Variational analysis},
  author={Rockafellar, R Tyrrell and Wets, Roger J-B},
  volume={317},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  number={28},
  year={1970},
  publisher={Princeton university press}
}



@article{punjani2017cryosparc,
  title={{cryoSPARC: algorithms for rapid unsupervised cryo-EM structure determination}},
  author={Punjani, Ali and Rubinstein, John L and Fleet, David J and Brubaker, Marcus A},
  journal={Nature Methods},
  volume={14},
  number={3},
  pages={290},
  year={2017},
  publisher={Nature Publishing Group}
}



@article{maaten2008visualizing,
  title={{Visualizing data using t-SNE}},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{wold1987principal,
  title={Principal component analysis},
  author={Wold, Svante and Esbensen, Kim and Geladi, Paul},
  journal={Chemometrics and Intelligent Laboratory Systems},
  volume={2},
  number={1-3},
  pages={37--52},
  year={1987},
  publisher={Elsevier}
}

@article{huang2019understanding,
  title={Understanding generalization through visualizations},
  author={Huang, W Ronny and Emam, Zeyad and Goldblum, Micah and Fowl, Liam and Terry, Justin K and Huang, Furong and Goldstein, Tom},
  journal={arXiv preprint arXiv:1906.03291},
  year={2019}
}

@inproceedings{liu2020on,
title={On the Variance of the Adaptive Learning Rate and Beyond},
author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgz2aEKDr}
}

@misc{mrtz,
  author={Hardt, Moritz },
  title = {Robustness versus acceleration},
  year={2014},
  howpublished = {\url{http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html}},
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{he@github,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Networks},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/KaimingHe/deep-residual-networks}}
}

@misc{bearpaw@github,
  author = {Yang, Wei},
  title = {Pytorch Classification},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bearpaw/pytorch-classification}}
}

@article{lu2017beyond,
  title={Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations},
  author={Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  journal={arXiv preprint arXiv:1710.10121},
  year={2017}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@inproceedings{zhang2015deep,
  title={Deep learning with elastic averaging SGD},
  author={Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={685--693},
  year={2015}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2595--2603},
  year={2010}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2019}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}




@inproceedings{roulet2017sharpness,
  title={Sharpness, restart and acceleration},
  author={Roulet, Vincent and d'Aspremont, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1119--1129},
  year={2017}
}

@article{devolder2014first,
  title={First-order methods of smooth convex optimization with inexact oracle},
  author={Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={146},
  number={1-2},
  pages={37--75},
  year={2014},
  publisher={Springer}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@inproceedings{Zhu2021LongShortTE,
  title={Long-Short Transformer: Efficient Transformers for Language and Vision},
  author={Chen Zhu and Wei Ping and Chaowei Xiao and Mohammad Shoeybi and Tom Goldstein and Anima Anandkumar and Bryan Catanzaro},
  booktitle={NeurIPS},
  year={2021}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}


@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages={308--318},
  year={2016}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}


@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}



@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. Akad. Nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@article{recht2010cs726,
  title={Cs726-lyapunov analysis and the heavy ball method},
  author={Recht, Benjamin},
  year={2010},
  publisher={Citeseer}
}

@article{dozat2016incorporating,
  title={{Incorporating Nesterov momentum into Adam}},
  author={Dozat, Timothy},
  year={2016}
}

@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{bhojanapalli2021eigen,
  title={Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Jain, Himanshu and Kumar, Sanjiv and Lukasik, Michal and Veit, Andreas},
  journal={arXiv preprint arXiv:2106.08823},
  year={2021}
}



@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}


% Sharpness assumption
@incollection{hoffman2003approximate,
  title={On approximate solutions of systems of linear inequalities},
  author={Hoffman, Alan J},
  booktitle={Selected Papers Of Alan J Hoffman: With Commentary},
  pages={174--176},
  year={2003},
  publisher={World Scientific}
}

@article{robinson1975application,
  title={An application of error bounds for convex programming in a linear space},
  author={Robinson, Stephen M},
  journal={SIAM Journal on Control},
  volume={13},
  number={2},
  pages={271--273},
  year={1975},
  publisher={SIAM}
}

@article{mangasarian1985condition,
  title={A condition number for differentiable convex inequalities},
  author={Mangasarian, Olvi L},
  journal={Mathematics of Operations Research},
  volume={10},
  number={2},
  pages={175--179},
  year={1985},
  publisher={INFORMS}
}

@article{auslender1988global,
  title={Global regularity theorems},
  author={Auslender, AA and Crouzeix, J-P},
  journal={Mathematics of Operations Research},
  volume={13},
  number={2},
  pages={243--253},
  year={1988},
  publisher={INFORMS}
}

%% This is most important for sharpness assumption
@inproceedings{lojasiewicz1993geometrie,
  title={Sur la geometrie semi-et sous-analytique},
  author={Lojasiewicz, Stanislas},
  booktitle={Annales de l'institut Fourier},
  volume={43},
  number={5},
  pages={1575--1595},
  year={1993}
}

% Restart
@article{nemirovskii1985optimal,
  title={Optimal methods of smooth convex minimization},
  author={Nemirovskii, Arkaddii S and Nesterov, Yu E},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={25},
  number={2},
  pages={21--30},
  year={1985},
  publisher={Elsevier}
}

@article{nesterov2013gradient,
  title={Gradient methods for minimizing composite functions},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={140},
  number={1},
  pages={125--161},
  year={2013},
  publisher={Springer}
}

@article{iouditski2014primal,
  title={Primal-dual subgradient methods for minimizing uniformly convex functions},
  author={Iouditski, Anatoli and Nesterov, Yuri},
  journal={arXiv preprint arXiv:1401.1792},
  year={2014}
}

@inproceedings{lin2014adaptive,
  title={An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization},
  author={Lin, Qihang and Xiao, Lin},
  booktitle={International Conference on Machine Learning},
  pages={73--81},
  year={2014}
}

@article{renegar2014efficient,
  title={Efficient first-order methods for linear programming and semidefinite programming},
  author={Renegar, James},
  journal={arXiv preprint arXiv:1409.5832},
  year={2014}
}

@article{freund2018new,
  title={New computational guarantees for solving convex optimization problems with first order methods, via a function growth condition measure},
  author={Freund, Robert M and Lu, Haihao},
  journal={Mathematical Programming},
  volume={170},
  number={2},
  pages={445--477},
  year={2018},
  publisher={Springer}
}

@article{roulet2015computational,
  title={Computational complexity versus statistical performance on sparse recovery problems},
  author={Roulet, Vincent and Boumal, Nicolas and d'Aspremont, Alexandre},
  journal={arXiv preprint arXiv:1506.03295},
  year={2015}
}

@article{o2015adaptive,
  title={Adaptive restart for accelerated gradient schemes},
  author={O’donoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of Computational Mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer}
}

@inproceedings{giselsson2014monotonicity,
  title={Monotonicity and restart in fast gradient methods},
  author={Giselsson, Pontus and Boyd, Stephen},
  booktitle={53rd IEEE Conference on Decision and Control},
  pages={5058--5063},
  year={2014},
  organization={IEEE}
}

@inproceedings{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2510--2518},
  year={2014}
}


% Momentum
@inproceedings{zhang2019lookahead,
  title={Lookahead Optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9593--9604},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}

@inproceedings{darken1992towards,
  title={Towards faster stochastic gradient search},
  author={Darken, Christian and Moody, John},
  booktitle={Advances in neural information processing systems},
  pages={1009--1016},
  year={1992}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}

@article{cohen2018acceleration,
  title={On acceleration with noise-corrupted gradients},
  author={Cohen, Michael B and Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1805.12591},
  year={2018}
}

@article{aybat2018robust,
  title={Robust accelerated gradient methods for smooth strongly convex functions},
  author={Aybat, Necdet Serhat and Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asuman},
  journal={arXiv preprint arXiv:1805.10579},
  year={2018}
}

@article{kulunchakov2019estimate,
  title={Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise},
  author={Kulunchakov, Andrei and Mairal, Julien},
  journal={arXiv preprint arXiv:1901.08788},
  year={2019}
}

@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  year={1847}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{goh2017momentum,
  title={Why momentum really works},
  author={Goh, Gabriel},
  journal={Distill},
  volume={2},
  number={4},
  pages={e6},
  year={2017}
}

@inproceedings{bengio2013advances,
  title={Advances in optimizing recurrent networks},
  author={Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8624--8628},
  year={2013},
  organization={IEEE}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2017-12-14T14:05:27.000+0100},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/slicside},
  description = {Nurfür Referenzzwecke verwendet (MNIST)},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {ba-2018-hahnrico dropout final uw_ws17_ml},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2018-12-03T15:56:49.000+0100},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@article{osher2018laplacian,
  title={Laplacian smoothing gradient descent},
  author={Osher, Stanley and Wang, Bao and Yin, Penghang and Luo, Xiyang and Barekat, Farzin and Pham, Minh and Lin, Alex},
  journal={arXiv preprint arXiv:1806.06317},
  year={2018}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@article{jin2017accelerated,
  title={Accelerated gradient descent escapes saddle points faster than gradient descent},
  author={Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
  journal={arXiv preprint arXiv:1711.10456},
  year={2017}
}

@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={699--707},
  year={2016}
}

@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}


@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}



@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@inproceedings{gulrajani2017improved,
  title={{Improved training of Wasserstein GANs}},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5767--5777},
  year={2017}
}

@article{seeger2004gaussian,
  title={Gaussian processes for machine learning},
  author={Seeger, Matthias},
  journal={International journal of neural systems},
  volume={14},
  number={02},
  pages={69--106},
  year={2004},
  publisher={World Scientific}
}
@inproceedings{tsai-etal-2019-transformer,
    title = "Transformer Dissection: An Unified Understanding for Transformer{'}s Attention via the Lens of Kernel",
    author = "Tsai, Yao-Hung Hubert  and
      Bai, Shaojie  and
      Yamada, Makoto  and
      Morency, Louis-Philippe  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1443",
    doi = "10.18653/v1/D19-1443",
    pages = "4344--4353",
    abstract = "Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer{'}s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer{'}s attention. As an example, we propose a new variant of Transformer{'}s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
}

@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}












@article{ho2019axial,
  title={Axial Attention in Multidimensional Transformers},
  author={Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  journal={arXiv preprint arXiv:1912.12180},
  year={2019}
}

@article{liu2018generating,
  title={Generating wikipedia by summarizing long sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  journal={arXiv preprint arXiv:1801.10198},
  year={2018}
}

@article{qiu2019blockwise,
  title={Blockwise Self-Attention for Long Document Understanding},
  author={Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie},
  journal={arXiv preprint arXiv:1911.02972},
  year={2019}
}

@article{Kitaev2020Reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}


@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}


@article{carion2020end,
  title={End-to-End Object Detection with Transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  journal={arXiv preprint arXiv:2005.12872},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{tsai2019transformer,
  title={Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{ahmed2017weighted,
  title={Weighted transformer network for machine translation},
  author={Ahmed, Karim and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02132},
  year={2017}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}

@inproceedings{dai2019transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}


@article{nguyen2021fmmformer,
  title={FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention},
  author={Tan M. Nguyen and Vai Suliafu and Stanley J. Osher and Long Chen and Bao Wang},
  journal={arXiv preprint arXiv:2108.02347},
  year={2021}
}


@article{so2019evolved,
  title={The evolved transformer},
  author={So, David R and Liang, Chen and Le, Quoc V},
  journal={arXiv preprint arXiv:1901.11117},
  year={2019}
}



@article{asai2020challenges,
  title={Challenges in Information Seeking QA: Unanswerable Questions and Paragraph Retrieval},
  author={Asai, Akari and Choi, Eunsol},
  journal={arXiv preprint arXiv:2010.11915},
  year={2020}
}

@article{sukhbaatar2019augmenting,
  title={Augmenting self-attention with persistent memory},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:1907.01470},
  year={2019}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International Conference on Machine Learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}




@misc{dtrivgithub,
  author = {Casado, Mario Lezcano},
  title = {Optimization with orthogonal constraints and on general manifolds},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Lezcano/expRNN}}
}

@misc{ptbgithub,
  author = {Salesforce},
  title = {LSTM and QRNN Language Model Toolkit for PyTorch},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/salesforce/awd-lstm-lm}}
}


@misc{Tieleman2012,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@article{garofolo1993timit,
  title={TIMIT acoustic phonetic continuous speech corpus},
  author={Garofolo, John S},
  journal={Linguistic Data Consortium, 1993},
  year={1993}
}


@inproceedings{kusupati2018fastgrnn,
  title={Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network},
  author={Kusupati, Aditya and Singh, Manish and Bhatia, Kush and Kumar, Ashish and Jain, Prateek and Varma, Manik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9017--9028},
  year={2018}
}

@inproceedings{chandar2019towards,
  title={Towards non-saturating recurrent units for modelling long-term dependencies},
  author={Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3280--3287},
  year={2019}
}

@article{van2018unreasonable,
  title={The unreasonable effectiveness of the forget gate},
  author={Van Der Westhuizen, Jos and Lasenby, Joan},
  journal={arXiv preprint arXiv:1804.04849},
  year={2018}
}


@article{merityRegOpt,
  title={{Regularizing and Optimizing LSTM Language Models}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}
@misc{mrtz,
  author={Hardt, Moritz },
  title = {Robustness versus acceleration},
  year={2014},
  howpublished = {\url{http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html}},
}
@article{merityAnalysis,
  title={{An Analysis of Neural Language Modeling at Multiple Scales}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1803.08240},
  year={2018}
}

@article{moreau2017understanding,
  title={Understanding the learned iterative soft thresholding algorithm with matrix factorization},
  author={Moreau, Thomas and Bruna, Joan},
  journal={arXiv preprint arXiv:1706.01338},
  year={2017}
}


@article{lecun2010mnist,
  title={{MNIST handwritten digit database}},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}
@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}
@article{bjerrum2017molecular,
  title={Molecular generation with recurrent neural networks (RNNs)},
  author={Bjerrum, Esben Jannik and Threlfall, Richard},
  journal={arXiv preprint arXiv:1705.04612},
  year={2017}
}

@inproceedings{zhou2018sc2net,
  title={SC2Net: Sparse LSTMs for sparse coding},
  author={Zhou, Joey Tianyi and Di, Kai and Du, Jiawei and Peng, Xi and Yang, Hao and Pan, Sinno Jialin and Tsang, Ivor W and Liu, Yong and Qin, Zheng and Goh, Rick Siow Mong},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}


@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Eleventh Annual Conference of the International Speech Communication Association},
  year={2010}
}

@InProceedings{pmlr-v48-henaff16,
  title = 	 {Recurrent Orthogonal Networks and Long-Memory Tasks},
  author = 	 {Mikael Henaff and Arthur Szlam and Yann LeCun},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2034--2042},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/henaff16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/henaff16.html}
}

@inproceedings{mhammedi2017efficient,
  title={Efficient orthogonal parametrisation of recurrent neural networks using householder reflections},
  author={Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2401--2409},
  year={2017},
  organization={JMLR. org}
}

@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}



@article{hochreiter1997long,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural Computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1120--1128},
  year={2016}
}
@article{kotsias2019direct,
  title={Direct Steering of de novo Molecular Generation using Descriptor Conditional Recurrent Neural Networks (cRNNs)},
  author={Kotsias, Panagiotis-Christos and Ar{\'u}s-Pous, Josep and Chen, Hongming and Engkvist, Ola and Tyrchan, Christian and Bjerrum, Esben Jannik},
  year={2019},
  publisher={ChemRxiv}
}


@article{goh2017momentum,
  title={Why momentum really works},
  author={Goh, Gabriel},
  journal={Distill},
  volume={2},
  number={4},
  pages={e6},
  year={2017}
}


@inproceedings{bengio2013advances,
  title={Advances in optimizing recurrent networks},
  author={Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8624--8628},
  year={2013},
  organization={IEEE}
}




@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@article{kag2019rnns,
  title={{RNNs} Evolving in Equilibrium: A Solution to the Vanishing and Exploding Gradients},
  author={Kag, Anil and Zhang, Ziming and Saligrama, Venkatesh},
  journal={arXiv preprint arXiv:1908.08574},
  year={2019}
}

@article{miller2018stable,
  title={Stable recurrent models},
  author={Miller, John and Hardt, Moritz},
  journal={arXiv preprint arXiv:1805.10369},
  year={2018}
}

@article{chen2019symplectic,
  title={Symplectic Recurrent Neural Networks},
  author={Chen, Zhengdao and Zhang, Jianyu and Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1909.13334},
  year={2019}
}

@article{chang2019antisymmetricrnn,
  title={AntisymmetricRNN: A dynamical system view on recurrent neural networks},
  author={Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H},
  journal={arXiv preprint arXiv:1902.09689},
  year={2019}
}

@inproceedings{casado2019trivializations,
  title={Trivializations for gradient-based optimization on manifolds},
  author={Casado, Mario Lezcano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9154--9164},
  year={2019}
}
@article{he2019momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  journal={arXiv preprint arXiv:1911.05722},
  year={2019}
}

@inproceedings{jing2017tunable,
  title={Tunable efficient unitary neural networks (eunn) and their application to rnns},
  author={Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Solja{\v{c}}i{\'c}, Marin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1733--1741},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{merity2018regularizing,
title={Regularizing and Optimizing {LSTM} Language Models},
author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyyGPP0TZ},
}

@inproceedings{cooijmans2016recurrent,
  title={Recurrent batch normalization},
  author={Tim Cooijmans and Nicolas Ballas and C{\'e}sar Laurent and {\c{C}}a{\u{g}}lar G{\"u}l{\c{c}}ehre and Aaron Courville},
  booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems},
  pages={2378--2386},
  year={2016}
}

@inproceedings{lezcano2019cheap,
  title={Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group},
  author={Lezcano-Casado, Mario and Mart{\'i}nez-Rubio, David},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={3794--3803},
  year={2019}
}

@inproceedings{wisdom2016full,
  title={Full-capacity unitary recurrent neural networks},
  author={Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4880--4888},
  year={2016}
}
@inproceedings{shen2021efficient,
  title={Efficient attention: Attention with linear complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3531--3539},
  year={2021}
}
@inproceedings{vorontsov2017orthogonality,
  title={On orthogonality and learning recurrent networks with long term dependencies},
  author={Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3570--3578},
  year={2017},
  organization={JMLR. org}
}
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{van2020enhancing,
  title={Enhancing lexical-based approach with external knowledge for Vietnamese multiple-choice machine reading comprehension},
  author={Van Nguyen, Kiet and Tran, Khiem Vinh and Luu, Son T and Nguyen, Anh Gia-Tuan and Nguyen, Ngan Luu-Thuy},
  journal={IEEE Access},
  volume={8},
  pages={201404--201417},
  year={2020},
  publisher={IEEE}
}

@inproceedings{lam2020uit,
  title={UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning},
  author={Lam, Quan Hoang and Le, Quang Duy and Nguyen, Van Kiet and Nguyen, Ngan Luu-Thuy},
  booktitle={International Conference on Computational Collective Intelligence},
  pages={730--742},
  year={2020},
  organization={Springer}
}

@inproceedings{Thn2017NGDH,
  title={Ung dung ho tro tra cuu cum tu dung trong bai bao khoa hoc bang tieng Anh},
  author={Dang Van Thin and Nguyen Van Kiet and Nguyen Luu Thuy Ngan,
  year={2017}
}
}

@inproceedings{ho2019emotion,
  title={Emotion recognition for vietnamese social media text},
  author={Ho, Vong Anh and Nguyen, Duong Huynh-Cong and Nguyen, Danh Hoang and Pham, Linh Thi-Van and Nguyen, Duc-Vu and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy},
  booktitle={International Conference of the Pacific Association for Computational Linguistics},
  pages={319--333},
  year={2019},
  organization={Springer}
}

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}


@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{song2019mass,
  title={Mass: Masked sequence to sequence pre-training for language generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1905.02450},
  year={2019}
}
@InProceedings{pmlr-v80-helfrich18a,
  title = 	 {Orthogonal Recurrent Neural Networks with Scaled {C}ayley Transform},
  author = 	 {Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1969--1978},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/helfrich18a/helfrich18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/helfrich18a.html},
  abstract = 	 {Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform; such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones. The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.}
}

@inproceedings{robinson1988static,
  title={Static and dynamic error propagation networks with application to speech coding},
  author={Robinson, Anthony J and Fallside, F},
  booktitle={Neural information processing systems},
  pages={632--641},
  year={1988}
}


@inproceedings{szlam2011structured,
  title={Structured sparse coding via lateral inhibition},
  author={Szlam, Arthur D and Gregor, Karol and Cun, Yann L},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1116--1124},
  year={2011}
}

@article{werbos1988generalization,
  title={Generalization of backpropagation with application to a recurrent gas market model},
  author={Werbos, Paul J},
  journal={Neural networks},
  volume={1},
  number={4},
  pages={339--356},
  year={1988},
  publisher={Elsevier}
}

@inproceedings{chalasani2013fast,
  title={A fast proximal method for convolutional sparse coding},
  author={Chalasani, Rakesh and Principe, Jose C and Ramakrishnan, Naveen},
  booktitle={The 2013 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--5},
  year={2013},
  organization={IEEE}
}

@article{mccann2017convolutional,
  title={Convolutional neural networks for inverse problems in imaging: A review},
  author={McCann, Michael T and Jin, Kyong Hwan and Unser, Michael},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={85--95},
  year={2017},
  publisher={IEEE}
}

@article{williams1995gradient,
  title={Gradient-based learning algorithms for recurrent},
  author={Williams, Ronald J and Zipser, David},
  journal={Backpropagation: Theory, architectures, and applications},
  volume={433},
  publisher={Psychology Press}
}

@article{neal2011mcmc,
  title={{MCMC} using {Hamiltonian} dynamics},
  author={Neal, Radford M and others}
}

@article{duane1987hybrid,
  title={Hybrid monte carlo},
  author={Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
  journal={Physics Letters B},
  volume={195},
  number={2},
  pages={216--222},
  year={1987},
  publisher={Elsevier}
}

@article{betancourt2017conceptual,
  title={A conceptual introduction to Hamiltonian Monte Carlo},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1701.02434},
  year={2017}
}

@article{kamilov2016learning,
  title={Learning MMSE Optimal Thresholds for FISTA},
  author={Kamilov, US and Mansour, H},
  year={2016}
}

@inproceedings{chen2014stochastic,
  title={Stochastic gradient hamiltonian monte carlo},
  author={Chen, Tianqi and Fox, Emily and Guestrin, Carlos},
  booktitle={International conference on machine learning},
  pages={1683--1691},
  year={2014}
}

@book{coffey2012langevin,
  title={The Langevin equation: with applications to stochastic problems in physics, chemistry and electrical engineering},
  author={Coffey, William and Kalmykov, Yu P},
  volume={27},
  year={2012},
  publisher={World Scientific}
}


@article{wang2020scheduled,
  title={Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent},
  author={Wang, Bao and Nguyen, Tan M and Bertozzi, Andrea L and Baraniuk, Richard G and Osher, Stanley J},
  journal={arXiv preprint arXiv:2002.10583},
  year={2020}
}


@article{nemirovskii1985optimal,
  title={Optimal methods of smooth convex minimization},
  author={Nemirovskii, Arkaddii S and Nesterov, Yu E},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={25},
  number={2},
  pages={21--30},
  year={1985},
  publisher={Elsevier}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. Akad. Nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}


@article{ranzato2014video,
  title={Video (language) modeling: a baseline for generative models of natural videos},
  author={Ranzato, MarcAurelio and Szlam, Arthur and Bruna, Joan and Mathieu, Michael and Collobert, Ronan and Chopra, Sumit},
  journal={arXiv preprint arXiv:1412.6604},
  year={2014}
}

@article{pearlmutter1989learning,
  title={Learning state space trajectories in recurrent neural networks},
  author={Pearlmutter, Barak A},
  journal={Neural Computation},
  volume={1},
  number={2},
  pages={263--269},
  year={1989},
  publisher={MIT Press}
}

@article{cauwenberghs1996analog,
  title={An analog VLSI recurrent neural network learning a continuous-time trajectory},
  author={Cauwenberghs, Gert},
  journal={IEEE Transactions on Neural Networks},
  volume={7},
  number={2},
  pages={346--361},
  year={1996},
  publisher={IEEE}
}

@inproceedings{gallagher2005reconfigurable,
  title={A reconfigurable continuous time recurrent neural network for evolvable hardware applications},
  author={Gallagher, John C and Boddhu, Sanjay K and Vigraham, Saranyan},
  booktitle={2005 IEEE Congress on Evolutionary Computation},
  volume={3},
  pages={2461--2468},
  year={2005},
  organization={IEEE}
}

@inproceedings{gregor2010learning,
  title={Learning fast approximations of sparse coding},
  author={Gregor, Karol and LeCun, Yann},
  booktitle={Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages={399--406},
  year={2010}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive Science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@incollection{jordan1990attractor,
  title={Attractor dynamics and parallelism in a connectionist sequential machine},
  author={Jordan, Michael I},
  booktitle={Artificial neural networks: concept learning},
  pages={112--127},
  year={1990}
}

@inproceedings{chen1996comparative,
  title={A comparative study of recurrent neural network architectures on learning temporal sequences},
  author={Chen, Tung-Bo and Soo, Von-Wun},
  booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)},
  volume={4},
  pages={1945--1950},
  year={1996},
  organization={IEEE}
}

@article{vster2013selective,
  title={Selective recurrent neural network},
  author={{\v{S}}ter, Branko},
  journal={Neural processing letters},
  volume={38},
  number={1},
  pages={1--15},
  year={2013},
  publisher={Springer}
}

@inproceedings{zhang-feng-2021-modeling-concentrated,
    title = "Modeling Concentrated Cross-Attention for Neural Machine Translation with {G}aussian Mixture Model",
    author = "Zhang, Shaolei  and
      Feng, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.121",
    doi = "10.18653/v1/2021.findings-emnlp.121",
    pages = "1401--1411",
    abstract = "Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central words and then spreads around them. In this work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation.",
}

@article{lu2021fpt,
  title={Pretrained Transformers as Universal Computation Engines},
  author={Kevin Lu and Aditya Grover and Pieter Abbeel and Igor Mordatch},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@inproceedings{huang2021speech,
  title={Speech recognition by simply fine-tuning BERT},
  author={Huang, Wen-Chin and Wu, Chia-Hua and Luo, Shang-Bao and Chen, Kuan-Yu and Wang, Hsin-Min and Toda, Tomoki},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7343--7347},
  year={2021},
  organization={IEEE}
}

@inproceedings{nguyen2022improving,
  title={Improving transformers with probabilistic attention keys},
  author={Nguyen, Tam Minh and Nguyen, Tan Minh and Le, Dung DD and Nguyen, Duy Khuong and Tran, Viet-Anh and Baraniuk, Richard and Ho, Nhat and Osher, Stanley},
  booktitle={International Conference on Machine Learning},
  pages={16595--16621},
  year={2022},
  organization={PMLR}
}

@article{talathi2015improving,
  title={Improving performance of recurrent neural network with relu nonlinearity},
  author={Talathi, Sachin S and Vartak, Aniket},
  journal={arXiv preprint arXiv:1511.03771},
  year={2015}
}

@article{niu2019recurrent,
  title={Recurrent neural networks in the eye of differential equations},
  author={Niu, Murphy Yuezhen and Horesh, Lior and Chuang, Isaac},
  journal={arXiv preprint arXiv:1904.12933},
  year={2019}
}

@article{laurent2016recurrent,
  title={A recurrent neural network without chaos},
  author={Laurent, Thomas and von Brecht, James},
  journal={arXiv preprint arXiv:1612.06212},
  year={2016}
}

@inproceedings{fernandez2007sequence,
  title={Sequence labelling in structured domains with hierarchical recurrent neural networks},
  author={Fern{\'a}ndez, Santiago and Graves, Alex and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 20th International Joint Conference on Artificial Intelligence, IJCAI 2007},
  year={2007}
}

@article{hsu2016exploiting,
  title={Exploiting depth and highway connections in convolutional recurrent deep neural networks for speech recognition},
  author={Hsu, Wei-Ning and Zhang, Yu and Lee, Ann and Glass, James},
  journal={cell},
  volume={50},
  pages={1},
  year={2016}
}

@article{sak2014long,
  title={Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition},
  author={Sak, Ha{\c{s}}im and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1402.1128},
  year={2014}
}

@inproceedings{qu2017syllable,
  title={{Syllable-based acoustic modeling with CTC-SMBR-LSTM}},
  author={Qu, Zhongdi and Haghani, Parisa and Weinstein, Eugene and Moreno, Pedro},
  booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={173--177},
  year={2017},
  organization={IEEE}
}

@inproceedings{altche2017lstm,
  title={An LSTM network for highway trajectory prediction},
  author={Altch{\'e}, Florent and de La Fortelle, Arnaud},
  booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={353--359},
  year={2017},
  organization={IEEE}
}

@article{palangi2016deep,
  title={Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval},
  author={Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={4},
  pages={694--707},
  year={2016},
  publisher={IEEE}
}

@article{mallinar2018deep,
  title={Deep canonically correlated LSTMs},
  author={Mallinar, Neil and Rosset, Corbin},
  journal={arXiv preprint arXiv:1801.05407},
  year={2018}
}

@article{karpathy2015visualizing,
  title={Visualizing and understanding recurrent networks},
  author={Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1506.02078},
  year={2015}
}

@inproceedings{li2018independently,
  title={Independently recurrent neural network (indrnn): Building a longer and deeper rnn},
  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5457--5466},
  year={2018}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE Transactions on Neural Networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1310--1318},
  year={2013}
}


@article{gers1999learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year={1999},
  publisher={IET}
}

@article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{gers2000recurrent,
  title={Recurrent nets that time and count},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  volume={3},
  pages={189--194},
  year={2000},
  organization={IEEE}
}

@article{yu2019review,
  title={A review of recurrent neural networks: LSTM cells and network architectures},
  author={Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  journal={Neural Computation},
  volume={31},
  number={7},
  pages={1235--1270},
  year={2019},
  publisher={MIT Press}
}

@article{gers2001lstm,
  title={{LSTM recurrent networks learn simple context-free and context-sensitive languages}},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}

@article{gers2002learning,
  title={Learning precise timing with LSTM recurrent networks},
  author={Gers, Felix A and Schraudolph, Nicol N and Schmidhuber, J{\"u}rgen},
  journal={Journal of machine learning research},
  volume={3},
  number={Aug},
  pages={115--143},
  year={2002}
}

@article{greff2016lstm,
  title={LSTM: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{weiss2018practical,
  title={On the practical computational power of finite precision RNNs for language recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  journal={arXiv preprint arXiv:1805.04908},
  year={2018}
}

@article{britz2017massive,
  title={Massive exploration of neural machine translation architectures},
  author={Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  journal={arXiv preprint arXiv:1703.03906},
  year={2017}
}

@article{zhou2015c,
  title={A C-LSTM neural network for text classification},
  author={Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis},
  journal={arXiv preprint arXiv:1511.08630},
  year={2015}
}

@inproceedings{neil2016phased,
  title={{Phased LSTM: Accelerating recurrent network training for long or event-based sequences}},
  author={Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3882--3890},
  year={2016}
}


@inproceedings{MomentumRNN,
  title={{MomentumRNN: Integrating Momentum into Recurrent Neural Networks}},
  author={Tan Nguyen and Richard Baraniuk and Andrea Bertozzi and Stanley Osher and Bao Wang},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS 2020)},
  year={2020}
}




@inproceedings{nina2015simplified,
  title={Simplified LSTM unit and search space probability exploration for image description},
  author={Nina, Oliver and Rodriguez, Andres},
  booktitle={2015 10th International Conference on Information, Communications and Signal Processing (ICICS)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@inproceedings{rahman2016new,
  title={{A new LSTM model by introducing biological cell state}},
  author={Rahman, Lamia and Mohammed, Nabeel and Al Azad, Abul Kalam},
  booktitle={2016 3rd International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)},
  pages={1--6},
  year={2016},
  organization={IEEE}
}

@inproceedings{pulver2017lstm,
  title={{LSTM with working memory}},
  author={Pulver, Andrew and Lyu, Siwei},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={845--851},
  year={2017},
  organization={IEEE}
}

@article{jing2019gated,
  title={Gated orthogonal recurrent units: On learning to forget},
  author={Jing, Li and Gulcehre, Caglar and Peurifoy, John and Shen, Yichen and Tegmark, Max and Soljacic, Marin and Bengio, Yoshua},
  journal={Neural Computation},
  volume={31},
  number={4},
  pages={765--783},
  year={2019},
  publisher={MIT Press}
}


@inproceedings{irie2016lstm,
  title={LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition.},
  author={Irie, Kazuki and T{\"u}ske, Zolt{\'a}n and Alkhouli, Tamer and Schl{\"u}ter, Ralf and Ney, Hermann},
  year={2016}
}

@inproceedings{veeriah2015differential,
  title={Differential recurrent neural networks for action recognition},
  author={Veeriah, Vivek and Zhuang, Naifan and Qi, Guo-Jun},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4041--4049},
  year={2015}
}

@inproceedings{nie2016bidirectional,
  title={A bidirectional LSTM model for question title and body analysis in question answering},
  author={Nie, Yuanping and An, Chao and Huang, Jiuming and Yan, Zhou and Han, Yi},
  booktitle={2016 IEEE First International Conference on Data Science in Cyberspace (DSC)},
  pages={307--311},
  year={2016},
  organization={IEEE}
}

@inproceedings{du2017stacked,
  title={Stacked LSTM deep learning model for traffic prediction in vehicle-to-vehicle communication},
  author={Du, Xunsheng and Zhang, Huaqing and Van Nguyen, Hien and Han, Zhu},
  booktitle={2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}


@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}

@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, A. and Schmidhuber, J.},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005}
}

@article{thireou2007bidirectional,
  title={Bidirectional long short-term memory networks for predicting the subcellular localization of eukaryotic proteins},
  author={Thireou, Trias and Reczko, Martin},
  journal={IEEE/ACM transactions on computational biology and bioinformatics},
  volume={4},
  number={3},
  pages={441--446},
  year={2007},
  publisher={IEEE}
}

@article{wu2016empirical,
  title={An empirical exploration of skip connections for sequential tagging},
  author={Wu, Huijia and Zhang, Jiajun and Zong, Chengqing},
  journal={arXiv preprint arXiv:1610.03167},
  year={2016}
}

@inproceedings{graves2007multi,
  title={Multi-dimensional recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Schmidhuber, J{\"u}rgen},
  booktitle={International conference on artificial neural networks},
  pages={549--558},
  year={2007},
  organization={Springer}
}

@inproceedings{li2016exploring,
  title={Exploring multidimensional LSTMs for large vocabulary ASR},
  author={Li, Jinyu and Mohamed, Abdelrahman and Zweig, Geoffrey and Gong, Yifan},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4940--4944},
  year={2016},
  organization={IEEE}
}

@inproceedings{li2015lstm,
  title={LSTM time and frequency recurrence for automatic speech recognition},
  author={Li, Jinyu and Mohamed, Abdelrahman and Zweig, Geoffrey and Gong, Yifan},
  booktitle={2015 IEEE workshop on automatic speech recognition and understanding (ASRU)},
  pages={187--191},
  year={2015},
  organization={IEEE}
}

@article{oord2016pixel,
  title={Pixel recurrent neural networks},
  author={Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1601.06759},
  year={2016}
}

@article{shabanian2017variational,
  title={Variational bi-lstms},
  author={Shabanian, Samira and Arpit, Devansh and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1711.05717},
  year={2017}
}

@inproceedings{liang2016semantic,
  title={Semantic object parsing with local-global long short-term memory},
  author={Liang, Xiaodan and Shen, Xiaohui and Xiang, Donglai and Feng, Jiashi and Lin, Liang and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3185--3193},
  year={2016}
}

@inproceedings{liang2016semantic1,
  title={Semantic object parsing with graph lstm},
  author={Liang, Xiaodan and Shen, Xiaohui and Feng, Jiashi and Lin, Liang and Yan, Shuicheng},
  booktitle={European Conference on Computer Vision},
  pages={125--143},
  year={2016},
  organization={Springer}
}

@inproceedings{goller1996learning,
  title={Learning task-dependent distributed representations by backpropagation through structure},
  author={Goller, Christoph and Kuchler, Andreas},
  booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)},
  volume={1},
  pages={347--352},
  year={1996},
  organization={IEEE}
}

@inproceedings{liang2017interpretable,
  title={Interpretable structure-evolving LSTM},
  author={Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1010--1019},
  year={2017}
}

@article{kalchbrenner2015grid,
  title={Grid long short-term memory},
  author={Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
  journal={arXiv preprint arXiv:1507.01526},
  year={2015}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@inproceedings{xingjian2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Xingjian, SHI and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  booktitle={Advances in neural information processing systems},
  pages={802--810},
  year={2015}
}

@article{yao2015depth,
  title={Depth-gated LSTM},
  author={Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
  journal={arXiv preprint arXiv:1508.03790},
  year={2015}
}

@inproceedings{chung2015gated,
  title={Gated feedback recurrent neural networks},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2067--2075},
  year={2015}
}

@inproceedings{zhu2015long,
  title={Long short-term memory over recursive structures},
  author={Zhu, Xiaodan and Sobihani, Parinaz and Guo, Hongyu},
  booktitle={International Conference on Machine Learning},
  pages={1604--1612},
  year={2015}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@article{werbos1990backpropagation,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}






















@book{bebendorf2008hierarchical,
	author = {Bebendorf, Mario},
	publisher = {Springer},
	title = {Hierarchical Matrices},
	year = {2008}}

@article{xiSuperfastStableStructured2014,
	author = {Xi, Yuanzhe and Xia, Jianlin and Cauley, Stephen and Balakrishnan, Venkataramanan},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	month = jan,
	number = {1},
	pages = {44--72},
	title = {Superfast and {{Stable Structured Solvers}} for {{Toeplitz Least Squares}} via {{Randomized Sampling}}},
	volume = {35},
	year = {2014}}

@article{hackbusch2004hierarchical,
  title={Hierarchical matrices based on a weak admissibility criterion},
  author={Hackbusch, Wolfgang and Khoromskij, Boris N and Kriemann, Ronald},
  journal={Computing},
  volume={73},
  number={3},
  pages={207--243},
  year={2004},
  publisher={Springer}
}

@article{hackbusch2002data,
  title={Data-sparse approximation by adaptive $\mathcal{H}^2$-matrices},
  author={Hackbusch, Wolfgang and B{\"o}rm, Steffen},
  journal={Computing},
  volume={69},
  number={1},
  pages={1--35},
  year={2002},
  publisher={Springer}
}

@article{xia2010fast,
  title={Fast algorithms for hierarchically semiseparable matrices},
  author={Xia, Jianlin and Chandrasekaran, Shivkumar and Gu, Ming and Li, Xiaoye S},
  journal={Numerical Linear Algebra with Applications},
  volume={17},
  number={6},
  pages={953--976},
  year={2010},
  publisher={Wiley Online Library}
}

@article{chandrasekaran2006fast,
  title={A fast {ULV} decomposition solver for hierarchically semiseparable representations},
  author={Chandrasekaran, Shiv and Gu, Ming and Pals, Timothy},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={28},
  number={3},
  pages={603--622},
  year={2006},
  publisher={SIAM}
}

@article{ambikasaran2013mathcal,
  title={An $\mathcal{O}(N\log N)$ Fast Direct Solver for Partial Hierarchically Semi-Separable Matrices},
  author={Ambikasaran, Sivaram and Darve, Eric},
  journal={Journal of Scientific Computing},
  volume={57},
  number={3},
  pages={477--501},
  year={2013},
  publisher={Springer}
}
@article{martinsson2005fast,
  title={A fast direct solver for boundary integral equations in two dimensions},
  author={Martinsson, Per-Gunnar and Rokhlin, Vladimir},
  journal={Journal of Computational Physics},
  volume={205},
  number={1},
  pages={1--23},
  year={2005},
  publisher={Elsevier}
}

@article{chandrasekaran2007fast,
  title={A fast solver for {HSS} representations via sparse matrices},
  author={Chandrasekaran, Shiv and Dewilde, Patrick and Gu, Ming and Lyons, William and Pals, Timothy},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={29},
  number={1},
  pages={67--81},
  year={2007},
  publisher={SIAM}
}

@InProceedings{H2matrix,
author="Hackbusch, W.
and Khoromskij, B.
and Sauter, S. A.",
editor="Bungartz, Hans-Joachim
and Hoppe, Ronald H. W.
and Zenger, Christoph",
title="On $\mathcal{H}^2$-Matrices",
booktitle="Lectures on Applied Mathematics",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--29",
abstract="A class of matrices (H-matrices) has recently been introduced by one of the authors. These matrices have the following properties: (i) They are sparse in the sense that only few data are needed for their representation, (ii) The matrix-vector multiplication is of almost linear complexity, (iii) In general, sums and products of these matrices are no longer in the same set, but their truncations to the H-matrix format are again of almost linear complexity, (iv) The same statement holds for the inverse of an H-matrix.",
isbn="978-3-642-59709-1"
}


@article{hackbusch1999sparse,
  title={A sparse matrix arithmetic based on $\mathcal{H}$-matrices. part I: Introduction to $\mathcal{H}$-matrices},
  author={Hackbusch, Wolfgang},
  journal={Computing},
  volume={62},
  number={2},
  pages={89--108},
  year={1999},
  publisher={Springer}
}

@article{van2014accelerating,
  title={Accelerating t-SNE using tree-based algorithms},
  author={Van Der Maaten, Laurens},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={3221--3245},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{lee2012distributed,
  title={A distributed kernel summation framework for general-dimension machine learning},
  author={Lee, Dongryeol and Vuduc, Richard and Gray, Alexander G},
  booktitle={Proceedings of the 2012 SIAM International Conference on Data Mining},
  pages={391--402},
  year={2012},
  organization={SIAM}
}

@article{gray2001n,
  title={N-Body'problems in statistical learning},
  author={Gray, Alexander G and Moore, Andrew W},
  journal={Advances in neural information processing systems},
  pages={521--527},
  year={2001},
  publisher={Citeseer}
}
@inproceedings{
peng2021random,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}

@article{greengard1987fast,
  title={A fast algorithm for particle simulations},
  author={Greengard, Leslie and Rokhlin, Vladimir},
  journal={Journal of computational physics},
  volume={73},
  number={2},
  pages={325--348},
  year={1987},
  publisher={Elsevier}
}


@article{Lee:18,
  author    = {Jason Lee and Elman Mansimov and Kyunghyun Cho},
  title     = {Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement},
  year      = {2018},
  journal   = {arXiv preprint arXiv:1802.06901},
}

@article{bengio2019consciousness,
      title={The Consciousness Prior}, 
      author={Yoshua Bengio},
      year={2019},
      eprint={1709.08568},
      archivePrefix={arXiv},
      journal   = {arXiv preprint arXiv:1709.08568},
      primaryClass={cs.LG}
}


@InProceedings{pmlr-v37-xuc15, title = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}, author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {2048--2057}, year = {2015}, editor = {Bach, Francis and Blei, David}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/xuc15.pdf}, url = { http://proceedings.mlr.press/v37/xuc15.html }, abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.} }

@inproceedings{
geng2021is,
title={Is Attention Better Than Matrix Decomposition?},
author={Zhengyang Geng and Meng-Hao Guo and Hongxu Chen and Xia Li and Ke Wei and Zhouchen Lin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=1FvkSpWosOl}
}


@article{DDD04,
	Author = {Daubechies, I. and Defrise, M. and De Mol, C.},
	Issn = {1097-0312},
	Journal = {Communications on Pure and Applied Mathematics},
	Keywords = {parid, sparsity},
	Number = {11},
	Pages = {1413--1457},
	Title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
	Volume = {57},
	Year = {2004}}

@article{Fu98,
	Author = {Fu, W.},
	Issue = {3},
	Journal = {JCGS},
	Pages = {397--416},
	Title = {Penalized Regressions: The Bridge vs the lasso},
	Volume = {7},
	Year = {1998}}

@article{Tibshirani94,
	Author = {Robert Tibshirani},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Pages = {267--288},
	Title = {Regression Shrinkage and Selection Via the Lasso},
	Volume = {58},
	Year = {1994}}

@article{CDS98,
	Author = {Scott Shaobing Chen and David L. Donoho and Michael A. Saunders},
	Journal = {SIAM Journal on Scientific Computing},
	Pages = {33--61},
	Title = {Atomic Decomposition by Basis Pursuit},
	Volume = {20},
	Year = {1998}}

@inproceedings{BSLRBS07,
	Address = {Miami, Florida},
	Author = {Baker, S. and Scharstein, D. and Lewis, J.P. and Roth, S. and Black, M.J. and Szeliski, R.},
	Booktitle = {IEEE 11th International Conference on Computer Vision, 2007.},
	Title = {A Database and Evaluation Methodology for Optical Flow},
	Year = {2007}}

@techreport{PCBC09,
	Author = {Pock, Thomas and Cremers, Daniel and Bischof, Horst and Chambolle, Antonin},
	Institution = {Institute for Computer Graphics and Vision, Graz University of Technology},
	Title = {Global Solutions of Variational Models with Convex Regularization},
	Year = {2009}}

@techreport{CCP:opt,
	Author = {Chambolle, Antonin and Cremers, Daniel and Pock, Thomas},
	Institution = {\\ tre des Mathematiques Appliquees, Ecole Polytechnique, Palaiseau, Paris, France},
	Owner = {Jakob},
	Timestamp = {2008.11.28},
	Title = {A convex approach for computing minimal partitions},
	Year = {2008}}

@inproceedings{PCBC09_relax,
	Address = {Miami, Florida},
	Author = {Pock, T. and Chambolle, A. and Bischof, H. and Cremers, D.},
	Booktitle = {I{EEE} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition (CVPR)},
	Keywords = {total-variation},
	Title = {A Convex Relaxation Approach for Computing Minimal Partitions},
	Year = {2009}}
	
@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22--31},
  year={2021}
}

@inproceedings{woo2018cbam,
  title={Cbam: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{EZC09,
	author = {Ernie Esser and Xiaoqun Zhang and Tony F. Chan},
title = {A General Framework for a Class of First Order Primal-Dual Algorithms for Convex Optimization in Imaging Science},
journal = {SIAM Journal on Imaging Sciences},
volume = {3},
number = {4},
pages = {1015-1046},
year = {2010}}

@article{BCB09,
	Author = {Brown,Ethan S. and Chan, Tony F. and Bresson, Xavier},
	Journal = {UCLA CAM Report 09-66},
	Month = {July},
	Title = {Convex Formulation and Exact Global Solutions for Multi-phase Piecewise Constant Mumford-Shah Image Segmentation},
	Year = {2009}}

@book{Federer69,
	Author = {Federer, Herbert},
	Publisher = {Springer-Verlag},
	Title = {Geometric measure theory},
	Year = {1969}}

@article{DS06,
	Address = {Norwell, MA, USA},
	Author = {Darbon, J\'{e}r\^{o}me and Sigelle, Marc},
	Doi = {http://dx.doi.org/10.1007/s10851-006-0644-3},
	Issn = {0924-9907},
	Journal = {J. Math. Imaging Vis.},
	Number = {3},
	Pages = {277--291},
	Publisher = {Kluwer Academic Publishers},
	Title = {Image Restoration with Discrete Constrained Total Variation Part II: Levelable Functions, Convex Priors and Non-Convex Cases},
	Volume = {26},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10851-006-0644-3}}

@article{DS06_cuts,
	Address = {Norwell, MA, USA},
	Author = {Darbon, J\'{e}r\^{o}me and Sigelle, Marc},
	Doi = {http://dx.doi.org/10.1007/s10851-006-8803-0},
	Issn = {0924-9907},
	Journal = {J. Math. Imaging Vis.},
	Number = {3},
	Pages = {261--276},
	Publisher = {Kluwer Academic Publishers},
	Title = {Image Restoration with Discrete Constrained Total Variation Part I: Fast and Exact Optimization},
	Volume = {26},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10851-006-8803-0}}

@article{FF:gc,
	Author = {Ford, L. R. and Fulkerson, D. R.},
	Journal = {Canadian Journal of Mathematics},
	Pages = {399--404},
	Title = {Maximal flow through a network},
	Volume = {8},
	Year = {1956}}

@article{Potts52,
	Author = {Potts, Renfrey B.},
	Journal = {Proc. of the Cambridge Philosophical Society},
	Pages = {106--109},
	Title = {Some Generalized Order-Disorder Transformations},
	Volume = {48},
	Year = {1952}}

@article{GO07,
	Author = {Gilboa, Guy and Osher, Stanley},
	Issue = {2},
	Journal = {Multiscale Model. Simul.},
	Month = {July},
	Pages = {595--630},
	Title = {Nonlocal Linear Image Regularization and Supervised Segmentation},
	Volume = {6},
	Year = {2007}}

@article{Darbon09,
	Author = {Darbon, J\'{e}r\^{o}me},
	Doi = {10.1016/j.dam.2009.02.026},
	Issn = {0166218X},
	Journal = {Discrete Applied Mathematics},
	Month = {June},
	Posted-At = {2009-07-11 22:14:14},
	Title = {Global optimization for first order Markov Random Fields with submodular priors},
	Url = {http://dx.doi.org/10.1016/j.dam.2009.02.026},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.dam.2009.02.026}}

@article{Schrijver00,
	Address = {Orlando, FL, USA},
	Author = {Schrijver, Alexander},
	Doi = {http://dx.doi.org/10.1006/jctb.2000.1989},
	Issn = {0095-8956},
	Journal = {J. Comb. Theory Ser. B},
	Number = {2},
	Pages = {346--355},
	Publisher = {Academic Press, Inc.},
	Title = {A combinatorial algorithm minimizing submodular functions in strongly polynomial time},
	Volume = {80},
	Year = {2000},
	Bdsk-Url-1 = {http://dx.doi.org/10.1006/jctb.2000.1989}}

@article{IFF00,
	Author = {Satoru IWATA and Lisa FLEISCHER and Satoru FUJISHIGE},
	Journal = {Journal of the ACM},
	Pages = {761--777},
	Title = {A Combinatorial Strongly Polynomial Algorithm for Minimizing Submodular Functions},
	Volume = {48},
	Year = {2000}}

@article{Ishikawa03,
	Author = {Hiroshi Ishikawa},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Pages = {1333--1336},
	Title = {Exact optimization for markov random fields with convex priors},
	Volume = {25},
	Year = {2003}}

@inproceedings{Jacquot08,
	Author = {Jie Zhu-Jacquot},
	Booktitle = {Image Analysis and Interpretation, 2008. SSIAI 2008. IEEE Southwest Symposium on},
	Doi = {10.1109/SSIAI.2008.4512297},
	Keywords = {image segmentation, medical image processingcardiac images, graph cuts segmentation, kidney images, medical images, shape priors, shape segmentation},
	Month = {March},
	Pages = {109-112},
	Title = {Graph Cuts Segmentation with Geometric Shape Priors for Medical Images},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/SSIAI.2008.4512297}}

@inproceedings{BJ01,
	Author = {Boykov, Y.Y. and Jolly, M.-P.},
	Booktitle = {Computer Vision, 2001. ICCV 2001. Proc.. Eighth IEEE International Conference on},
	Doi = {10.1109/ICCV.2001.937505},
	Keywords = {computational geometry, image segmentation, interactive systemsGestalt example, N-D images, N-dimensional images, globally optimal segmentation, hard constraints, interactive graph cuts, interactive segmentation, max-flow algorithm, medical image segmentation, optimal boundary &, region segmentation, soft constraints},
	Pages = {105-112 vol.1},
	Title = {Interactive graph cuts for optimal boundary region segmentation of objects in N-D images},
	Volume = {1},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCV.2001.937505}}

@article{BF06,
	Address = {Hingham, MA, USA},
	Author = {Boykov, Yuri and Funka-Lea, Gareth},
	Doi = {http://dx.doi.org/10.1007/s11263-006-7934-5},
	Issn = {0920-5691},
	Journal = {Int. J. Comput. Vision},
	Number = {2},
	Pages = {109--131},
	Publisher = {Kluwer Academic Publishers},
	Title = {Graph Cuts and Efficient N-D Image Segmentation},
	Volume = {70},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11263-006-7934-5}}

@article{GPS89,
	Author = {Greig, D. M. and Porteous, B. T. and Seheult, A. H.},
	Journal = {Journal of the Royal Statistics Society},
	Keywords = {bayesian-statistics, graph-theory, vision},
	Number = {2},
	Pages = {271--279},
	Posted-At = {2009-01-02 05:24:24},
	Priority = {2},
	Title = {Exact Maximum A Posteriori Estimation for Binary Images},
	Url = {http://www.jstor.org/stable/2345609},
	Volume = {51},
	Year = {1989},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2345609}}

@article{KGV83,
	Author = {S. Kirkpatrick and C. D. Gelatt and M. P. Vecchi},
	Journal = {Science},
	Pages = {671--680},
	Title = {Optimization by simulated annealing},
	Volume = {220},
	Year = {1983}}

@article{BK04,
	Address = {Washington, DC, USA},
	Author = {Boykov, Yuri and Kolmogorov, Vladimir},
	Doi = {http://dx.doi.org/10.1109/TPAMI.2004.60},
	Issn = {0162-8828},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {9},
	Pages = {1124--1137},
	Publisher = {IEEE Computer Society},
	Title = {An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision},
	Volume = {26},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.2004.60}}

@article{BVZ01,
	Address = {Washington, DC, USA},
	Author = {Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
	Doi = {http://dx.doi.org/10.1109/34.969114},
	Issn = {0162-8828},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {11},
	Pages = {1222--1239},
	Publisher = {IEEE Computer Society},
	Title = {Fast Approximate Energy Minimization via Graph Cuts},
	Volume = {23},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/34.969114}}

@book{BZ87,
	Address = {Cambridge, MA, USA},
	Author = {Blake, Andrew and Zisserman, Andrew},
	Isbn = {0-262-02271-0},
	Publisher = {MIT Press},
	Title = {Visual reconstruction},
	Year = {1987}}

@article{Besag86,
	Author = {Besag, Julian},
	Doi = {10.2307/2345426},
	Issn = {00359246},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Number = {3},
	Pages = {259--302},
	Posted-At = {2008-11-16 13:36:51},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {On the Statistical Analysis of Dirty Pictures},
	Url = {http://dx.doi.org/10.2307/2345426},
	Volume = {48},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.2307/2345426}}

@article{GGAHK93,
	Author = {Geman, Stuart and Geman, Donald and Abend, K. and Harley, T. J. and Kanal, L. N.},
	Journal = {Journal of Applied Statistics},
	Keywords = {gibbs, sampling},
	Number = {5},
	Pages = {25--62},
	Posted-At = {2009-06-16 16:58:20},
	Priority = {2},
	Publisher = {Routledge},
	Title = {Stochastic relaxation, Gibbs distributions and the {Bayesian} restoration of images},
	Volume = {20},
	Year = {1993}}

@article{Besag74,
	Author = {Besag, Julian},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Keywords = {1974, bayesian, disease\_mapping, gmrf, intrinsic, statistics},
	Number = {2},
	Pages = {192--136},
	Posted-At = {2008-09-22 10:29:45},
	Priority = {2},
	Title = {Spatial Interaction and the Statistical Analysis of Lattice Systems},
	Volume = {36},
	Year = {1974}}

@book{Baxter07,
	Author = {Baxter, Rodney J.},
	Isbn = {0486462714},
	Keywords = {book, ising, physics},
	Month = {December},
	Posted-At = {2007-12-14 05:18:51},
	Priority = {2},
	Publisher = {{Dover Publications}},
	Title = {Exactly Solved Models in Statistical Mechanics},
	Year = {2007}}

@article{WS01,
	Address = {Hingham, MA, USA},
	Author = {Weickert, Joachim and Schn\"{o}rr, Christoph},
	Doi = {http://dx.doi.org/10.1023/A:1013614317973},
	Issn = {0920-5691},
	Journal = {Int. J. Comput. Vision},
	Number = {3},
	Pages = {245--264},
	Publisher = {Kluwer Academic Publishers},
	Title = {A Theoretical Framework for Convex Regularizers in PDE-Based Computation of Image Motion},
	Volume = {45},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1013614317973}}

@inproceedings{PGPO94,
	Address = {London, UK},
	Author = {Proesmans, Marc and Gool, Luc J. Van and Pauwels, Eric J. and Oosterlinck, Andr\'{e}},
	Booktitle = {ECCV '94: Proc. of the Third European Conference-Volume II on Computer Vision},
	Isbn = {3-540-57957-5},
	Pages = {295--304},
	Publisher = {Springer-Verlag},
	Title = {Determination of Optical Flow and its Discontinuities using Non-Linear Diffusion},
	Year = {1994}}

@article{NE86,
	Address = {Washington, DC, USA},
	Author = {Nagel, H H and Enkelmann, W},
	Doi = {http://dx.doi.org/10.1109/TPAMI.1986.4767833},
	Issn = {0162-8828},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {5},
	Pages = {565--593},
	Publisher = {IEEE Computer Society},
	Title = {An investigation of smoothness constraints for the estimation of displacement vector fields from image sequences},
	Volume = {8},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.1986.4767833}}

@article{HB93,
	Address = {Washington, DC, USA},
	Author = {Heitz, F. and Bouthemy, P.},
	Doi = {http://dx.doi.org/10.1109/34.250841},
	Issn = {0162-8828},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {12},
	Pages = {1217--1232},
	Publisher = {IEEE Computer Society},
	Title = {Multimodal Estimation of Discontinuous Optical Flow using Markov Random Fields},
	Volume = {15},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/34.250841}}

@inproceedings{Cohen93,
	Address = {Tromso, Norway},
	Author = {Cohen, Isaac},
	Booktitle = {Proc. Eighth Scandinavian Conference on Image Analysis},
	Month = {May},
	Pages = {523--530},
	Title = {Nonlinear Variational Method for Optical Flow Computation},
	Volume = {1},
	Year = {1993}}

@inproceedings{BWKS05,
	Booktitle = {Scale-Space and PDE Methods in Computer Vision, volume 3459 of Lecture Notes in Computer Science},
	Pages = {279--290},
	Publisher = {Springer},
	Title = {Discontinuity-preserving computation of variational optic flow in real-time},
	Year = {2005}}



@inproceedings{DAHL06,
	Author = {Duan, Q. and Angelini, E. and Homma, S. and Laine, A.},
	Booktitle = {Engineering in Medicine and Biology Society, 2006. EMBS '06. 28th Annual International Conference of the IEEE},
	Doi = {10.1109/IEMBS.2006.260172},
	Issn = {1557-170X},
	Keywords = {biomechanics, biomedical MRI, cardiology, image motion analysis, image sequences, medical image processing, muscleCartesian coordinates, Lucas-Kanade optical flow method, cardiac image analysis, cardiac wall motion analysis, cine cardiac MRI series, echocardiography, endocardial surface, endocardium tracking, iso-value curve, myocardial motion estimation, myocardium deformation, three-dimensional ultrasound},
	Month = {30 2006-Sept. 3},
	Pages = {707-710},
	Title = {Tracking Endocardium Using Optical Flow along Iso-Value Curve},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IEMBS.2006.260172}}

@article{KR05,
	Address = {Norwell, MA, USA},
	Author = {Keeling, Stephen L. and Ring, Wolfgang},
	Doi = {http://dx.doi.org/10.1007/s10851-005-4967-2},
	Issn = {0924-9907},
	Journal = {J. Math. Imaging Vis.},
	Number = {1},
	Pages = {47--65},
	Publisher = {Kluwer Academic Publishers},
	Title = {Medical Image Registration and Interpolation by Optical Flow with Maximal Rigidity},
	Volume = {23},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10851-005-4967-2}}

@article{HNWWKBJ00,
	Author = {N. Hata and A. Nabavi and W. Wells and S. Warfield and R. Kikinis and P. Black and F. Jolesz},
	Institution = {Department of Radiology, Brigham and Women's Hospital and Harvard Medical School, Boston, MA 02115, USA. noby@bwh.harvard.edu},
	Journal = {J Comput Assist Tomogr},
	Language = en_US,
	Month = 07,
	Pubmed = {10966182},
	Sponsor = {1P41 RR13218-01 (RR) funded by NCRR 5P01 CA67165-03 (CA) funded by NCI},
	Title = {Three-dimensional optical flow method for measurement of volumetric brain deformation from intraoperative MR images},
	Year = 2000}

@book{Rogers01,
	Address = {San Francisco, CA, USA},
	Author = {Rogers, David F.},
	Isbn = {1-55860-669-6},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {An introduction to NURBS: with historical perspective},
	Year = {2001}}

@article{JBHCMT05,
	Author = {Jonasson, Lisa and Bresson, Xavier and Hagmann, Patric and Cuisenaire, Olivier and Meuli , Reto and Thiran, Jean-Philippe},
	Journal = {Medical Image Analysis},
	Number = {9},
	Pages = {223--236},
	Title = {White matter fiber tract segmentation in {DT-MRI} using geometric flows},
	Volume = {9},
	Year = {2005}}

@inproceedings{MKASCS96,
	Address = {Washington, DC, USA},
	Author = {Malladi, R. and Kimmel, R. and Adalsteinsson, D. and Sapiro, G. and Caselles, V. and Sethian, J. A.},
	Booktitle = {MMBIA '96: Proc. of the 1996 Workshop on Mathematical Methods in Biomedical Image Analysis (MMBIA '96)},
	Isbn = {0-8186-7367-2},
	Pages = {244},
	Publisher = {IEEE Computer Society},
	Title = {A Geometric Approach to Segmentation and Analysis of 3D Medical Images},
	Year = {1996}}

@article{YKKOT97,
	Author = {Yezzi, A. and Kichenassamy, S. and Kumar, A. and Olver, P. and Tannenbaum, A.},
	Journal = {IEEE Trans. on Med. Imag.},
	Number = {2},
	Pages = {199--209},
	Title = {A geometric snake model for segmentation of medical imagery},
	Volume = {16},
	Year = {1997}}

@article{Boissonnat84,
	Address = {New York, NY, USA},
	Author = {Jean-Daniel Boissonnat},
	Issn = {0730-0301},
	Journal = {ACM Trans. Graph.},
	Number = {4},
	Pages = {266--286},
	Publisher = {ACM},
	Title = {Geometric structures for three-dimensional shape representation},
	Volume = {3},
	Year = {1984}}

@inproceedings{ABK98,
	Address = {New York, NY, USA},
	Author = {Nina Amenta and Marshall Bern and Manolis Kamvysselis},
	Booktitle = {SIGGRAPH '98: Proc. of the 25th annual conference on Computer graphics and interactive techniques},
	Pages = {415--421},
	Publisher = {ACM},
	Title = {A new Voronoi-based surface reconstruction algorithm},
	Year = {1998}}

@inproceedings{AB98,
	Address = {New York, NY, USA},
	Author = {Amenta, Nina and Bern, Marshall},
	Booktitle = {SCG '98: Proc. of the fourteenth annual symposium on Computational geometry},
	Keywords = {surface-reconstruction, voronoi},
	Pages = {39--48},
	Publisher = {ACM Press},
	Title = {Surface reconstruction by Voronoi filtering},
	Year = {1998}}

@inproceedings{ZOF01,
	Address = {Washington, DC, USA},
	Author = {Hong-Kai Zhao and Stanley Osher and Ronald Fedkiw},
	Booktitle = {VLSM '01: Proc. of the IEEE Workshop on Variational and Level Set Methods (VLSM'01)},
	Pages = {194},
	Publisher = {IEEE Computer Society},
	Title = {Fast Surface Reconstruction Using the Level Set Method},
	Year = {2001}}

@inproceedings{
van2018relational,
title={Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions},
author={Sjoerd van Steenkiste and Michael Chang and Klaus Greff and Jürgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryH20GbRW},
}

@inproceedings{10.5555/3295222.3295414,
author = {Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber, J\"{u}rgen},
title = {Neural Expectation Maximization},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects. We demonstrate that the learned representations are useful for next-step prediction.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6694–6704},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@inproceedings{
e2018matrix,
title={Matrix capsules with {EM} routing},
author={Geoffrey E Hinton and Sara Sabour and Nicholas Frosst},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJWLfGWRb},
}

@inproceedings{10.5555/3294996.3295142,
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
title = {Dynamic Routing between Capsules},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3859–3869},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}



@inproceedings{10.5555/3327757.3327951,
author = {Kosiorek, Adam R. and Kim, Hyunjik and Posner, Ingmar and Teh, Yee Whye},
title = {Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. It can reliably discover and track objects throughout the sequence of frames, and can also generate future frames conditioning on the current frame, thereby simulating expected motion of objects. This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et al., 2016), including learning in an unsupervised manner, and addresses its shortcomings. We use a moving multi-MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how SQAIR overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {8615–8625},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.5555/3326943.3327114,
author = {Liang, Xiaodan and hu, Zhiting and Zhang, Hao and Lin, Liang and Xing, Eric P.},
title = {Symbolic Graph Reasoning Meets Convolutions},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1858–1868},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@InProceedings{pmlr-v97-zhang19f, title = {{L}atent{GNN}: Learning Efficient Non-local Relations for Visual Recognition}, author = {Zhang, Songyang and He, Xuming and Yan, Shipeng}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {7374--7383}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/zhang19f/zhang19f.pdf}, url = { http://proceedings.mlr.press/v97/zhang19f.html }, abstract = {Capturing long-range dependencies in feature representations is crucial for many visual recognition tasks. Despite recent successes of deep convolutional networks, it remains challenging to model non-local context relations between visual features. A promising strategy is to model the feature context by a fully-connected graph neural network (GNN), which augments traditional convolutional features with an estimated non-local context representation. However, most GNN-based approaches require computing a dense graph affinity matrix and hence have difficulty in scaling up to tackle complex real-world visual problems. In this work, we propose an efficient and yet flexible non-local relation representation based on a novel class of graph neural networks. Our key idea is to introduce a latent space to reduce the complexity of graph, which allows us to use a low-rank representation for the graph affinity matrix and to achieve a linear complexity in computation. Extensive experimental evaluations on three major visual recognition tasks show that our method outperforms the prior works with a large margin while maintaining a low computation cost.} }


@inproceedings{10.5555/3157382.3157459,
author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
title = {Attend, Infer, Repeat: Fast Scene Understanding with Generative Models},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene -without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3233–3241},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}


@article{SSO94,
	Address = {San Diego, CA, USA},
	Author = {Sussman, Mark and Smereka, Peter and Osher, Stanley},
	Issn = {0021-9991},
	Journal = {J. Comput. Phys.},
	Number = {1},
	Pages = {146--159},
	Publisher = {Academic Press Professional, Inc.},
	Title = {A level set approach for computing solutions to incompressible two-phase flow},
	Volume = {114},
	Year = {1994}}

@article{HDDMS92,
	Author = {Hoppe, Hugues and Derose, Tony and Duchamp, Tom and Mcdonald, John and Stuetzle, Werner},
	Journal = {Computer Graphics},
	Keywords = {printed, surface-reconstruction},
	Number = {2},
	Pages = {71--78},
	Title = {Surface reconstruction from unorganized points},
	Volume = {26},
	Year = {1992}}

@article{ZOMK00,
	Author = {Hong-Kai Zhao and Stanley Osher and Barry Merriman and Myungjoo Kang},
	Journal = {Comput. Vis. Image Underst.},
	Number = {3},
	Pages = {295--314},
	Title = {Implicit and nonparametric shape reconstruction from unorganized data using a variational level set method},
	Volume = {80},
	Year = {2000}}

@article{AS95,
	Author = {Adalsteinsson D. and Sethian J.},
	Journal = {J. Comp. Physics},
	Pages = {269--277},
	Title = {A fast level set method for propagating interfaces},
	Volume = {118},
	Year = {1995}}

@article{AC05,
	Address = {Hingham, MA, USA},
	Author = {Jean-Fran\c{c}ois Aujol and Antonin Chambolle},
	Issn = {0920-5691},
	Journal = {Int. J. Comput. Vision},
	Number = {1},
	Pages = {85--104},
	Publisher = {Kluwer Academic Publishers},
	Title = {Dual Norms and Image Decomposition Models},
	Volume = {63},
	Year = {2005}}

@article{Chambolle04,
	Address = {Norwell, MA, USA},
	Author = {Antonin Chambolle},
	Issn = {0924-9907},
	Journal = {J. Math. Imaging Vis.},
	Number = {1-2},
	Pages = {89--97},
	Publisher = {Kluwer Academic Publishers},
	Title = {An Algorithm for Total Variation Minimization and Applications},
	Volume = {20},
	Year = {2004}}

@article{CGM99,
	Author = {Tony F. Chan and Gene H. Golub and Pep Mulet},
	Journal = {SIAM J. Sci. Comput},
	Pages = {1964--1977},
	Title = {A nonlinear primal-dual method for total variation-based image restoration},
	Volume = {20},
	Year = {1999}}

@article{CD08t,
	Author = {Antonin Chambolle and Jerome Darbon},
	Journal = {UCLA CAM Report 08-19},
	Title = {On Total Variation Minimization and Surface Evolution using Parametric Maximum Flows},
	Year = {2008}}

@article{WYYZ08,
	Author = {Yilun Wang and Junfeng Yang and Wotao Yin and Yin Zhang},
	Doi = {10.1137/080724265},
	Journal = {SIAM Journal on Imaging Sciences},
	Keywords = {half-quadratic; image deblurring; isotropic total variation; fast Fourier transform},
	Number = {3},
	Pages = {248-272},
	Publisher = {SIAM},
	Title = {A New Alternating Minimization Algorithm for Total Variation Image Reconstruction},
	Url = {http://link.aip.org/link/?SII/1/248/1},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {http://link.aip.org/link/?SII/1/248/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1137/080724265}}

@article{Yin09,
	Author = {Yin, Wotao.},
	Journal = {UCLA CAM technical report, 09-42},
	Title = {Analysis and Generalizations of the Linearized Bregman Method},
	Year = {2009}}

@article{Esser09,
	Author = {Esser, E.},
	Journal = {UCLA CAM technical report, 09-31},
	Title = {Applications of Lagrangian-Based Alternating Direction Methods and Connections to {S}plit {B}regman},
	Year = {2009}}

@article{ZC08,
	Author = {Zhu, Mingqiang and Chan, Tony},
	Journal = {UCLA CAM technical report, 08-34},
	Title = {An Efficient Primal-Dual Hybrid Gradient Algorithm for Total Variation Image Restoration},
	Year = {2008}}

@inproceedings{Setzer09,
	Author = {S. Setzer},
	Booktitle = {Proc. of Second International Conference on Scale Space Methods and Variational Methods in Computer Vision, SSVM 2009, Voss, Norway, June 1-5, 2009.},
	Editor = {A. Lie and M. Lysaker and K. Morken and X.-C. Tai},
	Pages = {464--476},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {Split {B}regman algorithm, {D}ouglas-{R}achford splitting and frame shrinkage},
	Volume = {5567},
	Year = {2009}}

@article{GO08,
	author = {Goldstein, Tom and Osher, Stanley},
 title = {The {S}plit {B}regman Method for $\ell_1$ Regularized Problems},
 journal = {SIAM Journal on Imaging Science},
 issue_date = {April 2009},
 volume = {2},
 number = {2},
 month = apr,
 year = {2009},
 issn = {1936-4954},
 pages = {323--343},
 numpages = {21},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
} 


@article{BH05,
	Author = {Burger, Martin and Hintermuller, Michael},
	Journal = {UCLA CAM technical report, 05-40},
	Title = {Projected Gradient Flows for BV / level set relaxation},
	Year = {2005}}

@article{BC07,
	Author = {Xavier Bresson and Tony Chan},
	Journal = {IEEE International Conference on Image Processing},
	Pages = {33--36},
	Title = {Active Contours Based on Chambolle's Mean Curvature Motion},
	Year = {2007}}

@article{Chambolle04:mc,
	Author = {Chambolle, A.},
	Journal = {Interfaces Free Bound},
	Number = {2},
	Pages = {195--218},
	Title = {An algorithm for mean curvature motion},
	Volume = {6},
	Year = {2004}}

@article{ATW93,
	Address = {Philadelphia, PA, USA},
	Author = {Fred Almgren and Jean E. Taylor and Lihe Wang},
	Issn = {0363-0129},
	Journal = {SIAM J. Control Optim.},
	Number = {2},
	Pages = {387--438},
	Publisher = {Society for Industrial and Applied Mathematics},
	Title = {Curvature-driven flows: a variational approach},
	Volume = {31},
	Year = {1993}}

@article{FH04,
	Address = {Hingham, MA, USA},
	Author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	Doi = {http://dx.doi.org/10.1023/B:VISI.0000022288.19776.77},
	Issn = {0920-5691},
	Journal = {Int. J. Comput. Vision},
	Number = {2},
	Pages = {167--181},
	Publisher = {Kluwer Academic Publishers},
	Title = {Efficient Graph-Based Image Segmentation},
	Volume = {59},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/B:VISI.0000022288.19776.77}}

@article{THMS05,
	Author = {Tschirren, J. and Hoffman, E.A. and McLennan, G. and Sonka, M.},
	Issue = {12},
	Journal = {Medical Imaging, IEEE Transactions on},
	Pages = {1529--1539},
	Title = {Intrathoracic airway trees: segmentation and airway morphology analysis from low-dose {CT} scans},
	Volume = {24},
	Year = {2005}}

@article{CBGM99,
	Author = {Chad Carson and Serge Belongie and Hayit Greenspan and Jitendra Malik},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Pages = {1026--1038},
	Title = {Blobworld: Image segmentation using Expectation-Maximization and its application to image querying},
	Volume = {24},
	Year = {1999}}

@article{BEVTO07,
	Author = {Xavier Bresson and Selim Esedoglu and Pierre Vandergheynst and Jean-Philippe 							Thiran and Stanley Osher},
	Issue = {2},
	Journal = {Journal of Mathematical Imaging and Vision},
	Pages = {151--167},
	Title = {Fast Global Minimization of the Active Contour/Snake Model},
	Volume = {28},
	Year = {2007}}

@book{Bertsekas96,
	Author = {Dimitri Bertsekas},
	Publisher = {Academic Press},
	Title = {Constrained Optimization and Lagrange Multiplier Methods},
	Year = {1996}}

 @article{DBLP:journals/corr/abs-2107-02192,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17723--17736},
  year={2021}
}

@article{CEN06,
	Author = {Tony Chan and Selim Esedoglu and Mila Nikolova},
	Issue = {5},
	Journal = {SIAM Journal on Applied Mathematics},
	Pages = {1932--1648},
	Title = {Algorithms for Finding Global Minimizers of Image Segmentation and Denoising Models},
	Volume = {66},
	Year = {2006}}

@article{CV01,
	Author = {Tony Chan and Luminita Vese},
	Issue = {2},
	Journal = {IEEE Transactions on Image Processing},
	Pages = {266--277},
	Title = {Active Contours Without Edges},
	Volume = {10},
	Year = {2001}}

@article{MS89,
	Author = {Mumford, D. and Shah, J.},
	Journal = {Comm. Pure Appl. Math.},
	Pages = {577--685},
	Title = {Optimal approximation by piecewise smooth functions and associated 				 variational problems},
	Volume = {42},
	Year = {1989}}

@book{OF03,
	Author = {Stanley Osher and Ronald Fedkiw},
	Publisher = {Springer Verlag},
	Title = {Level Set Methods and Dynamic Implicit Surfaces},
	Year = {2003}}

@inproceedings{Sethian99,
	Author = {J. A. Sethian},
	Booktitle = {Interfaces in Computational Geometry, Fluid Mechanics, Computer Vision, and Materials Science},
	Publisher = {Cambridge University Press},
	Title = {Level Set Methods and Fast Marching Methods: Evolving},
	Year = {1999}}

@techreport{WYGZ09,
	Author = {Zaiwen Wen and Wotao Yin and Donald Goldfarb and Yin Zheng},
	Institution = {Rice University},
	Title = {A Fast Algorithm for Sparse Reconstruction Based on Shrinkage, Subspace Optimization, and Continuation},
	Year = {2009}}

@techreport{WYG10,
	Author = {Zaiwen Wen and Wotao Yin and Donald Goldfarb},
	Institution = {Rice University},
	Title = {On the convergence of an active set method for $\ell_1$ minimization},
	Year = {2010}}

@article{OS88,
	Author = {Stanley Osher and James A. Sethian},
	Journal = {Journal of Computational Physics},
	Pages = {12--49},
	Title = {Fronts propagating with curvature dependent speed: algorithms based on Hamilton-Jacobi formulations},
	Volume = {79},
	Year = {1988}}

@article{KWT04,
	Author = {Kass, W. and Witkin, A. and Terzopoulos, D.},
	Journal = {International Journal of Computer Vision},
	Number = {4},
	Pages = {312-331},
	Title = {Snakes: Active Contour Models},
	Volume = {1},
	Year = {2004}}

@article{KS97,
	Author = {Ron Kimmel and Guillermo Sapiro},
	Journal = {International Journal of Computer Vision},
	Pages = {61--79},
	Title = {Geodesic active contours},
	Volume = {22},
	Year = {1997}}

@article{CKS95,
	Address = {Los Alamitos, CA, USA},
	Author = {V. Caselles and R. Kimmel and G. Sapiro},
	Journal = {Computer Vision, IEEE International Conference on},
	Pages = {694},
	Publisher = {IEEE Computer Society},
	Title = {Geodesic active contours},
	Volume = {0},
	Year = {1995}}

@incollection{Chambolle05,
	Author = {Chambolle, A.},
	Booktitle = {Energy Minimization Methods in Computer Vision and Pattern Recognition},
	Pages = {136 -- 152},
	Publisher = {Springer},
	Title = {Total Variation Minimization and a Class of Binary MRF models},
	Year = {2005}}

@misc{Yin_gc,
	Author = {Yin, W.},
	Title = {PGC: A Preflow-Push based Graph-Cut Solver. Version 2.32},
	Url = {http://www.caam.rice.edu/~optimization/L1/pgc/},
	Bdsk-Url-1 = {http://www.caam.rice.edu/~optimization/L1/pgc/}}

@article{GY08,
	Author = {Goldfarb, D. and Yin, W.},
	Journal = {CAAM technical report, TR07-09},
	Title = {Parametric Maximum Flow Algorithms for Fast Total Variation Minimization},
	Year = {2008}}

@article{PR75,
	Author = {Picard, J. C. and Ratliff, H. D.},
	Journal = {Networks},
	Number = {4},
	Pages = {357--370},
	Title = {Minimum cuts and related problems},
	Volume = {5},
	Year = {1975}}

@article{KZ02,
	Author = {Kolmogorov, Vladimir and Zabih, Ramin},
	Journal = {Proc. of Computer Vision - ECCV 2002: 7th European Conference on Computer Vision, Copenhagen, Denmark, May 28-31, 2002, Part III},
	Keywords = {cut, graph},
	Pages = {185--208},
	Title = {What Energy Functions Can Be Minimized via Graph Cuts?},
	Year = {2002}}

@article{BVZ:05,
	Author = {Boykov, Y. and Veksler, O. and Zabih, R.},
	Issue = {11},
	Journal = {Pattern Analysis and Machine Intelligence},
	Pages = {1222 -- 1239},
	Title = {Fast Approximate Energy Minimization via Graph Cuts},
	Volume = {23},
	Year = {2005}}

@article{Bregman67,
	Author = {Bregman, L},
	Journal = {USSR Computational Mathematics and Mathematical Physics},
	Pages = {200--217},
	Title = {The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex optimization},
	Volume = {7},
	Year = {1967}}

@inproceedings{BCM05,
	Author = {Buades, A. and Coll, B. and Morel, J. M.},
	Booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
	Doi = {10.1109/CVPR.2005.38},
	Journal = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
	Pages = {60--65 vol. 2},
	Posted-At = {2008-11-14 16:24:21},
	Title = {A non-local algorithm for image denoising},
	Volume = {2},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2005.38}}

@article{ROF92,
	Author = {Rudin, L and Osher, S and Fatemi, E},
	Journal = {Physica. D.},
	Pages = {259-268},
	Title = {Nonlinear total variation based noise removal algorithms},
	Volume = {60},
	Year = {1992}}

@article{WYZ07,
	Author = {Wang, Y and Yin, W and Zhang, Y},
	Journal = {CAAM Technical Reports},
	Title = {A Fast Algorithm for Image Deblurring with Total Variation Regularization},
	Year = {2007}}

@article{XO07,
	Author = {Xu, J and Osher, S},
	Issue = {2},
	Journal = {IEEE Transactions on Image Processing},
	Month = {February},
	Pages = {534--544},
	Title = {Iterative Regularization and Nonlinear Inverse Scale Space Applied to Wavelet-Based Denoising},
	Volume = {16},
	Year = {2007}}

@article{KB01,
	Author = {R. Kimmel and A. M. Bruckstein},
	Journal = {INTERNATIONAL JOURNAL OF COMPUTER VISION},
	Pages = {225--243},
	Title = {Regularized {L}aplacian Zero Crossings as Optimal Edge Integrators},
	Volume = {53},
	Year = {2001}}

@article{DJ95,
	Author = {Donoho, David L. and Johnstone, Iain M.},
	Journal = {Journal of the American Statistical Association},
	Keywords = {shrinkage, wavelet},
	Month = {December},
	Number = {432},
	Pages = {1200--1224},
	Posted-At = {2007-09-09 23:01:33},
	Priority = {2},
	Title = {Adapting to Unknown Smoothness via Wavelet Shrinkage},
	Volume = {90},
	Year = {1995}}

@inproceedings{CGM96,
	Address = {Berlin, Germany~/ Heidelberg, Germany~/ London, UK~/ etc.},
	Author = {Tony F. Chan and Gene H. Golub and Pep Mulet},
	Booktitle = {{ICAOS} '96 (Paris, 1996),},
	Pages = {241--252},
	Publisher = {Spring{\-}er-Ver{\-}lag},
	Title = {A nonlinear primal-dual method for total variation-based image restoration},
	Url = {citeseer.ist.psu.edu/article/chan95nonlinear.html},
	Volume = {219},
	Year = {1996},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/article/chan95nonlinear.html}}

@article{LS94,
	Author = {Li, Y and Santosa, F},
	Journal = {Tech Repost 12/94, Center for theory and simulation in science and engineering, Cornell University},
	Pages = {24},
	Title = {An affine scaling algorithm for minimizing total variation in image enhancement},
	Year = {1994}}

@article{LTS06,
	Address = {Seattle},
	Author = {Lin He and Ti-Chiun Chang and Stanley Osher},
	Journal = {Proc. of the 13th annual meeting of ISMRM},
	Pages = {696},
	Publisher = {Academic Press},
	Title = {MR Image Reconstruction from Sparse Radial Samples by Using Iterative Refinement Procedures},
	Year = 2006}

@article{VO96,
	Author = {C. R. Vogel and M. E. Oman},
	Journal = {SIAM Journal on Scientific Computing},
	Number = {1},
	Pages = {227--238},
	Title = {Iterative Methods for Total Variation Denoising},
	Volume = {17},
	Year = {1996}}

@article{Vogel95,
	Author = {C. Vogel},
	Journal = {Computation and Control IV, Progress in Systems and Control Theory, 20, Birkhauser},
	Title = {A multigrid method for total variation-based image denoising},
	Year = {1995}}

@inproceedings{PSGBC08,
	Address = {Berlin, Heidelberg},
	Author = {Pock, Thomas and Schoenemann, Thomas and Graber, Gottfried and Bischof, Horst and Cremers, Daniel},
	Booktitle = {ECCV '08: Proc. of the 10th European Conference on Computer Vision},
	Doi = {http://dx.doi.org/10.1007/978-3-540-88690-7_59},
	Isbn = {978-3-540-88689-1},
	Location = {Marseille, France},
	Pages = {792--805},
	Publisher = {Springer-Verlag},
	Title = {A Convex Formulation of Continuous Multi-label Problems},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-540-88690-7_59}}

@article{DS05,
	Author = {Jerome Darbon and Marc Sigelle},
	Journal = {IbPRIA 2005},
	Number = {1},
	Pages = {351--359},
	Title = {A Fast and Exact Algorithm for Total Variation Minimization},
	Volume = {3522},
	Year = {2005}}

@article{KZ04,
	Author = {Vladimir Kolmogorov and Ramin Zabih},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell},
	Pages = {147--159},
	Title = {What energy functions can be minimized via graph cuts},
	Year = {2004}}

@article{Setzer07,
	Author = {Simon Setzer},
	Journal = {International Journal of Computer Vision, to appear},
	Title = {Operator Splittings, Bregman Methods and Frame Shrinkage in Image Processing},
	Year = {2009}}

@article{HYY07,
	Author = {Elaine Hale and Wotao Yin and Yin Zhang},
	Journal = {CAAM Technical Report},
	Title = {A fixed-Point Continuation Method for $\ell_1$-Regularized Minimization with Applications to Compressed Sensing},
	Volume = {TR07},
	Year = {2007}}

@article{OBG05,
	Author = {Stanley Osher and Martin Burger and Donald Goldfarb and Jinjun Xu and Wotao Yin},
	Issue = {2},
	Journal = {MMS},
	Pages = {460-489},
	Title = {An Iterative Regularization Method for Total Variation-Based Image Restoration},
	Volume = {4},
	Year = {2005}}

@article{LLDP05,
	Author = {Lustig, M. and Lee, J. H. and Donoho, D. L. and Pauly, J. M.},
	Journal = {Proc. Of the ISMRM},
	Title = {Faster imaging with randomly perturbed undersampled spirals and $\ell_1$ reconstruction.},
	Year = {2005}}

@article{CRT06,
	Author = {E. J. Candes and J. Romberg and T.Tao},
	Journal = {IEEE Trans. Inform. Theory},
	Pages = {489 -- 509},
	Title = {Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information},
	Volume = {52},
	Year = {2006}}

@article{CR05,
	Author = {Candes, E. J. and Romberg, J.},
	Journal = {Proc. of SPIE Computational Imaging III},
	Pages = {76 -- 86},
	Title = {Signal recovery from random projections},
	Volume = {5674},
	Year = {2005}}

@article{Donoho06,
	Author = {Donoho, D.},
	Journal = {Information Theory, IEEE Transactions on},
	Pages = {1289--1306},
	Title = {Compressed Sensing},
	Volume = {52},
	Year = {2006}}

@article{LDP07,
	Author = {Lustig, M. and Donoho, D. and Pauly, J.},
	Issue = {6},
	Journal = {Magnetic Resonance in Medicine},
	Pages = {1182--1195},
	Title = {Sparse {MRI}: The application of compressed sensing for rapid {MR} imaging},
	Url = {www.stanford.edu/~mlustig/SparseMRI.pdf},
	Volume = {58},
	Year = {2007},
	Bdsk-Url-1 = {www.stanford.edu/~mlustig/SparseMRI.pdf}}

@article{TMB07,
	Author = {Trzasko, J and Manduca, A and Borisch, E},
	Journal = {Statistical Signal Processing},
	Pages = {176 -- 180},
	Title = {Sparse {MRI} reconstruction via multiscale L0-continuation},
	Volume = {26--29},
	Year = {2007}}

@article{BS07,
	Author = {Baraniuk, R. and Steeghs, P.},
	Journal = {Radar Conference, 2007 IEEE},
	Pages = {128--133},
	Title = {Compressive Radar Imaging},
	Year = {2007}}

@article{KKLBG07,
	Author = {Kim S. and Koh, K. and Lustig, M. and Boyd, S. and Gerinvesky, D.},
	Journal = {Tech. Report, Dept. of Electrical Engineering, Stanford University},
	Title = {A method for large-scale $\ell_1$-regularized least squares problems with applications in signal processing and statistics},
	Year = {2007}}

@article{YOGD08,
	Author = {Yin, W. and Osher, S. and Goldfarb, D. and Darbon, J.},
	Journal = {Siam J. Imaging Science},
	Pages = {142--168},
	Title = {{Bregman iterative algorithms for $\ell_1$-minimization with applications to compressed sensing}},
	Volume = {1},
	Year = {2008}}

@article{MDFMV96,
	Author = {Marseille, GL and de Beer, R and Fuderer, M and Mehlkopf, AF and van Ormondt, D},
	Journal = {J Magn Reson},
	Pages = {70--75},
	Title = {{Nonunifom phase-encode distributions for {MRI} scan-time reduction}},
	Volume = {111},
	Year = {1996}}

@article{COS08,
	Author = {Cai, JF and Osher, S. and Shen, Z},
	Journal = {UCLA CAM Report},
	Title = {{Linearized Bregman iterations for compressed sensing}},
	Year = {08}}

@article{OMDY08,
	Author = {Osher, A and Mao, Y and Dong, B and Yin, W},
	Journal = {UCLA CAM Report},
	Title = {{Fast linearized Bregman iterations for compressed sensing and sparse denoising}},
	Year = {08}}

@book{BV04,
	Author = {Stephen Boyd and Lieven Vandenberghe},
	Publisher = {Cambridge University Press},
	Title = {Convex Optimization},
	Year = {2004}}

@book{NW06,
	Author = {Nocedal, J and Wright, S.},
	Publisher = {Springer Verlag},
	Title = {Numerical Optimization},
	Year = {2006}}
@inproceedings{deng2019adaptive,
	title={An Adaptive Empirical Bayesian Method for Sparse Deep Learning},
	author={Deng, Wei and Zhang, Xiao and Liang, Faming and Lin, Guang},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5564--5574},
	year={2019}
}
@article{SGRLD,
	title={Stochastic gradient Riemannian Langevin dynamics on the probability simplex.},
	author={Patterson, Sam and Yee Whye Teh.},
	journal={In Advances in neural information processing systems},
	pages={3102-3110},
	year={2013}
}

@article{PSGLD,
	title={Preconditioned stochastic gradient Langevin dynamics for deep neural networks.},
	author={Li, Chunyuan and Changyou Chen and David Carlson and Lawrence Carin},
	journal={In Thirtieth AAAI Conference on Artificial Intelligence},
	year={2016}
}

@article{wang_multiphase,
	title={Efficient deep learning techniques for multiphase flow simulation in heterogeneous porousc media.},
	author={Wang, Yating and Guang Lin},
	journal={Journal of Computational Physics},
	volume={401},
	pages={108968},
	year={2020}
}

@article{efendiev2013generalized,
	title={Generalized multiscale finite element methods (GMsFEM)},
	author={Efendiev, Yalchin and Galvis, Juan and Hou, Thomas Y},
	journal={Journal of Computational Physics},
	volume={251},
	pages={116--135},
	year={2013},
	publisher={Elsevier}
}

@article{AI-alchemy,
author = {Hutson, Matthew},
year = {2018},
month = {05},
pages = {},
title = {{AI} researchers allege that machine learning is alchemy},
journal = {Science},
doi = {10.1126/science.aau0577}
}
@misc{
  sculley2018winner's,
  title={Winner's Curse?  On Pace, Progress, and Empirical Rigor},
  author={D. Sculley and Jasper Snoek and Alex Wiltschko and Ali Rahimi},
  year={2018},
  url={https://openreview.net/forum?id=rJWF0Fywf}
}


@article{hochreiter1997long,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural Computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}


@book{mordukhovich2006variational,
  title={Variational analysis and generalized differentiation I: Basic theory},
  author={Mordukhovich, Boris S},
  volume={330},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{nesterov1998introductory,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  year={1998}
}

@book{rockafellar2009variational,
  title={Variational analysis},
  author={Rockafellar, R Tyrrell and Wets, Roger J-B},
  volume={317},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  number={28},
  year={1970},
  publisher={Princeton university press}
}



@article{punjani2017cryosparc,
  title={{cryoSPARC: algorithms for rapid unsupervised cryo-EM structure determination}},
  author={Punjani, Ali and Rubinstein, John L and Fleet, David J and Brubaker, Marcus A},
  journal={Nature Methods},
  volume={14},
  number={3},
  pages={290},
  year={2017},
  publisher={Nature Publishing Group}
}



@article{maaten2008visualizing,
  title={{Visualizing data using t-SNE}},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{wold1987principal,
  title={Principal component analysis},
  author={Wold, Svante and Esbensen, Kim and Geladi, Paul},
  journal={Chemometrics and Intelligent Laboratory Systems},
  volume={2},
  number={1-3},
  pages={37--52},
  year={1987},
  publisher={Elsevier}
}

@article{huang2019understanding,
  title={Understanding generalization through visualizations},
  author={Huang, W Ronny and Emam, Zeyad and Goldblum, Micah and Fowl, Liam and Terry, Justin K and Huang, Furong and Goldstein, Tom},
  journal={arXiv preprint arXiv:1906.03291},
  year={2019}
}



@misc{mrtz,
  author={Hardt, Moritz },
  title = {Robustness versus acceleration},
  year={2014},
  howpublished = {\url{http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html}},
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{he@github,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Networks},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/KaimingHe/deep-residual-networks}}
}

@misc{bearpaw@github,
  author = {Yang, Wei},
  title = {Pytorch Classification},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bearpaw/pytorch-classification}}
}

@article{lu2017beyond,
  title={Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations},
  author={Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  journal={arXiv preprint arXiv:1710.10121},
  year={2017}
}

@inproceedings{NEURIPS2019_42a6845a,
 author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Latent Ordinary Differential Equations for Irregularly-Sampled Time Series},
 url = {https://proceedings.neurips.cc/paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf},
 volume = {32},
 year = {2019}
}
@inproceedings{
grathwohl2018scalable,
title={Scalable Reversible Generative Models with Free-form Continuous Dynamics},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}
@inproceedings{massaroli2020dissecting,
  title={Dissecting Neural ODEs},
  author={Massaroli, Stefano and Poli, Michael and Park, Jinkyoo and Yamashita, Atsushi and Asma, Hajime},
  booktitle={34th Conference on Neural Information Processing Systems, NeurIPS 2020},
  year={2020},
  organization={The Neural Information Processing Systems}
}
@inproceedings{NEURIPS2019_21be9a4b,
 author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Augmented Neural ODEs},
 url = {https://proceedings.neurips.cc/paper/2019/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf},
 volume = {32},
 year = {2019}
}
@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@techreport{rosenblatt1961principles,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc Buffalo NY}
}
@inproceedings{zhang2015deep,
  title={Deep learning with elastic averaging SGD},
  author={Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={685--693},
  year={2015}
}
@inproceedings{NEURIPS2019_99a40143,
 author = {Yildiz, Cagatay and Heinonen, Markus and Lahdesmaki, Harri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ODE2VAE: Deep generative second order ODEs with Bayesian neural networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/99a401435dcb65c4008d3ad22c8cdad0-Paper.pdf},
 volume = {32},
 year = {2019}
}
@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2595--2603},
  year={2010}
}
@article{cohen1983absolute,
  title={Absolute stability of global pattern formation and parallel memory storage by competitive neural networks},
  author={Cohen, Michael A and Grossberg, Stephen},
  journal={IEEE transactions on systems, man, and cybernetics},
  number={5},
  pages={815--826},
  year={1983},
  publisher={IEEE}
}
@inproceedings{norcliffe2020_sonode,
  title={On Second Order Behaviour in Augmented Neural ODEs},
  author={Alexander Norcliffe and Cristian Bodnar and Ben Day and Nikola Simidjievski and Pietro Li{\`o}},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2020}
}
@article{wilson2016lyapunov,
  title={A {L}yapunov analysis of momentum methods in optimization},
  author={Wilson, Ashia C and Recht, Benjamin and Jordan, Michael I},
  journal={arXiv preprint arXiv:1611.02635},
  year={2016}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}
@misc{lechner2020learning,
      title={Learning Long-Term Dependencies in Irregularly-Sampled Time Series}, 
      author={Mathias Lechner and Ramin Hasani},
      year={2020},
      eprint={2006.04418},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2019}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}




@inproceedings{roulet2017sharpness,
  title={Sharpness, restart and acceleration},
  author={Roulet, Vincent and d'Aspremont, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1119--1129},
  year={2017}
}



@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}


@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages={308--318},
  year={2016}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}


@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}


@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}

@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://www.aclweb.org/anthology/I05-5002",
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    url = "https://www.aclweb.org/anthology/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}


@inproceedings{NEURIPS2019_3416a75f,
 author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Stand-Alone Self-Attention in Vision Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf},
 volume = {32},
 year = {2019}
}


@INPROCEEDINGS{8237351,
  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Deformable Convolutional Networks}, 
  year={2017},
  volume={},
  number={},
  pages={764-773},
  doi={10.1109/ICCV.2017.89}}

@InProceedings{pmlr-v97-zhang19d, title = {Self-Attention Generative Adversarial Networks}, author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {7354--7363}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf}, url = { http://proceedings.mlr.press/v97/zhang19d.html }, abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work, boosting the best published Inception score from 36.8 to 52.52 and reducing Fréchet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.} }

@InProceedings{Zhu_2019_ICCV,
author = {Zhu, Zhen and Xu, Mengde and Bai, Song and Huang, Tengteng and Bai, Xiang},
title = {Asymmetric Non-Local Neural Networks for Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{lu2019understanding,
  title={Understanding and improving transformer from a multi-particle dynamic system point of view},
  author={Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1906.02762},
  year={2019}
}

@inproceedings{
tang2021probabilistic,
title={Probabilistic Transformer For Time Series Analysis},
author={Binh Tang and David S. Matteson},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=HfpNVDg3ExA}
}

@article{gabbur2021probabilistic,
  title={Probabilistic Attention for Interactive Segmentation},
  author={Gabbur, Prasad and Bilkhu, Manjot and Movellan, Javier},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7794--7803},
  year={2018}
}

@article{kreuzer2021rethinking,
  title={Rethinking graph transformers with spectral attention},
  author={Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and L{\'e}tourneau, Vincent and Tossou, Prudencio},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}

@InProceedings{pmlr-v162-sahiner22a,
  title = 	 {Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers},
  author =       {Sahiner, Arda and Ergen, Tolga and Ozturkler, Batu and Pauly, John and Mardani, Morteza and Pilanci, Mert},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19050--19088},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sahiner22a/sahiner22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sahiner22a.html},
  abstract = 	 {Vision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product self-attention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to block nuclear-norm regularization that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classification by fine-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.}
}

@inproceedings{sander2022sinkformers,
  title={Sinkformers: Transformers with doubly stochastic attention},
  author={Sander, Michael E and Ablin, Pierre and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3515--3530},
  year={2022},
  organization={PMLR}
}

@article{yang2022transformers,
  title={Transformers from an Optimization Perspective},
  author={Yang, Yongyi and Huang, Zengfeng and Wipf, David},
  journal={Conference on Neural Information Processing Systems},
  year={2022}
}

@InProceedings{Li_2019_ICCV,
author = {Li, Xia and Zhong, Zhisheng and Wu, Jianlong and Yang, Yibo and Lin, Zhouchen and Liu, Hong},
title = {Expectation-Maximization Attention Networks for Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}



@inproceedings{NEURIPS2018_e1654211,
 author = {Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A\^{}2-Nets: Double Attention Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/e165421110ba03099a1c0393373c5b43-Paper.pdf},
 volume = {31},
 year = {2018}
}

@InProceedings{Chen_2019_CVPR,
author = {Chen, Yunpeng and Rohrbach, Marcus and Yan, Zhicheng and Shuicheng, Yan and Feng, Jiashi and Kalantidis, Yannis},
title = {Graph-Based Global Reasoning Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}

@inproceedings{wiegreffe-pinter-2019-attention,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1002",
    doi = "10.18653/v1/D19-1002",
    pages = "11--20",
    abstract = "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model{'}s prediction, and consequently reach insights regarding the model{'}s decision-making process. A recent paper claims that {`}Attention is not Explanation{'} (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one{'}s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don{'}t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.",
}

@inproceedings{serrano-smith-2019-attention,
    title = "Is Attention Interpretable?",
    author = "Serrano, Sofia  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1282",
    doi = "10.18653/v1/P19-1282",
    pages = "2931--2951",
    abstract = "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components{'} representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components{'} overall importance to a model, it is by no means a fail-safe indicator.",
}

@INPROCEEDINGS{8578911,
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Non-local Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={7794-7803},
  doi={10.1109/CVPR.2018.00813}}

@article{recht2010cs726,
  title={Cs726-lyapunov analysis and the heavy ball method},
  author={Recht, Benjamin},
  year={2010},
  publisher={Citeseer}
}

@article{dozat2016incorporating,
  title={{Incorporating Nesterov momentum into Adam}},
  author={Dozat, Timothy},
  year={2016}
}

@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}







@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}



% Sharpness assumption
@incollection{hoffman2003approximate,
  title={On approximate solutions of systems of linear inequalities},
  author={Hoffman, Alan J},
  booktitle={Selected Papers Of Alan J Hoffman: With Commentary},
  pages={174--176},
  year={2003},
  publisher={World Scientific}
}

@article{robinson1975application,
  title={An application of error bounds for convex programming in a linear space},
  author={Robinson, Stephen M},
  journal={SIAM Journal on Control},
  volume={13},
  number={2},
  pages={271--273},
  year={1975},
  publisher={SIAM}
}

@article{mangasarian1985condition,
  title={A condition number for differentiable convex inequalities},
  author={Mangasarian, Olvi L},
  journal={Mathematics of Operations Research},
  volume={10},
  number={2},
  pages={175--179},
  year={1985},
  publisher={INFORMS}
}

@article{auslender1988global,
  title={Global regularity theorems},
  author={Auslender, AA and Crouzeix, J-P},
  journal={Mathematics of Operations Research},
  volume={13},
  number={2},
  pages={243--253},
  year={1988},
  publisher={INFORMS}
}

%% This is most important for sharpness assumption
@inproceedings{lojasiewicz1993geometrie,
  title={Sur la geometrie semi-et sous-analytique},
  author={Lojasiewicz, Stanislas},
  booktitle={Annales de l'institut Fourier},
  volume={43},
  number={5},
  pages={1575--1595},
  year={1993}
}

% Restart
@article{nemirovskii1985optimal,
  title={Optimal methods of smooth convex minimization},
  author={Nemirovskii, Arkaddii S and Nesterov, Yu E},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={25},
  number={2},
  pages={21--30},
  year={1985},
  publisher={Elsevier}
}

@article{nesterov2013gradient,
  title={Gradient methods for minimizing composite functions},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={140},
  number={1},
  pages={125--161},
  year={2013},
  publisher={Springer}
}

@article{iouditski2014primal,
  title={Primal-dual subgradient methods for minimizing uniformly convex functions},
  author={Iouditski, Anatoli and Nesterov, Yuri},
  journal={arXiv preprint arXiv:1401.1792},
  year={2014}
}

@inproceedings{lin2014adaptive,
  title={An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization},
  author={Lin, Qihang and Xiao, Lin},
  booktitle={International Conference on Machine Learning},
  pages={73--81},
  year={2014}
}

@article{renegar2014efficient,
  title={Efficient first-order methods for linear programming and semidefinite programming},
  author={Renegar, James},
  journal={arXiv preprint arXiv:1409.5832},
  year={2014}
}

@article{freund2018new,
  title={New computational guarantees for solving convex optimization problems with first order methods, via a function growth condition measure},
  author={Freund, Robert M and Lu, Haihao},
  journal={Mathematical Programming},
  volume={170},
  number={2},
  pages={445--477},
  year={2018},
  publisher={Springer}
}

@article{roulet2015computational,
  title={Computational complexity versus statistical performance on sparse recovery problems},
  author={Roulet, Vincent and Boumal, Nicolas and d'Aspremont, Alexandre},
  journal={arXiv preprint arXiv:1506.03295},
  year={2015}
}

@article{o2015adaptive,
  title={Adaptive restart for accelerated gradient schemes},
  author={O’donoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of Computational Mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer}
}



@inproceedings{giselsson2014monotonicity,
  title={Monotonicity and restart in fast gradient methods},
  author={Giselsson, Pontus and Boyd, Stephen},
  booktitle={53rd IEEE Conference on Decision and Control},
  pages={5058--5063},
  year={2014},
  organization={IEEE}
}

@inproceedings{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2510--2518},
  year={2014}
}


% Momentum
@inproceedings{zhang2019lookahead,
  title={Lookahead Optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9593--9604},
  year={2019}
}



@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}

@inproceedings{darken1992towards,
  title={Towards faster stochastic gradient search},
  author={Darken, Christian and Moody, John},
  booktitle={Advances in neural information processing systems},
  pages={1009--1016},
  year={1992}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}

@article{cohen2018acceleration,
  title={On acceleration with noise-corrupted gradients},
  author={Cohen, Michael B and Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1805.12591},
  year={2018}
}

@article{aybat2018robust,
  title={Robust accelerated gradient methods for smooth strongly convex functions},
  author={Aybat, Necdet Serhat and Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asuman},
  journal={arXiv preprint arXiv:1805.10579},
  year={2018}
}

@article{kulunchakov2019estimate,
  title={Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise},
  author={Kulunchakov, Andrei and Mairal, Julien},
  journal={arXiv preprint arXiv:1901.08788},
  year={2019}
}

@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  year={1847}
}



@article{goh2017momentum,
  title={Why momentum really works},
  author={Goh, Gabriel},
  journal={Distill},
  volume={2},
  number={4},
  pages={e6},
  year={2017}
}



@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2017-12-14T14:05:27.000+0100},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/slicside},
  description = {Nurfür Referenzzwecke verwendet (MNIST)},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {ba-2018-hahnrico dropout final uw_ws17_ml},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2018-12-03T15:56:49.000+0100},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@article{osher2018laplacian,
  title={Laplacian smoothing gradient descent},
  author={Osher, Stanley and Wang, Bao and Yin, Penghang and Luo, Xiyang and Barekat, Farzin and Pham, Minh and Lin, Alex},
  journal={arXiv preprint arXiv:1806.06317},
  year={2018}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@article{jin2017accelerated,
  title={Accelerated gradient descent escapes saddle points faster than gradient descent},
  author={Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
  journal={arXiv preprint arXiv:1711.10456},
  year={2017}
}

@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={699--707},
  year={2016}
}

@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}


@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}



@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@inproceedings{gulrajani2017improved,
  title={{Improved training of Wasserstein GANs}},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5767--5777},
  year={2017}
}


@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}





















@article{carion2020end,
  title={End-to-End Object Detection with Transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  journal={arXiv preprint arXiv:2005.12872},
  year={2020}
}


@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and et al.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}


@article{ahmed2017weighted,
  title={Weighted transformer network for machine translation},
  author={Ahmed, Karim and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02132},
  year={2017}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}



@inproceedings{
Clark2020ELECTRA:,
title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}

@inproceedings{NEURIPS2019_e43739bb,
 author = {Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sampled Softmax with Random Fourier Features},
 url = {https://proceedings.neurips.cc/paper/2019/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2019_2c601ad9,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{chen2021chasing,
  title={Chasing sparsity in vision transformers: An end-to-end exploration},
  author={Chen, Tianlong and et al.},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{zhang-etal-2020-ternarybert,
    title = "{T}ernary{BERT}: Distillation-aware Ultra-low Bit {BERT}",
    author = "Zhang, Wei  and
      Hou, Lu  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Chen, Xiao  and
      Jiang, Xin  and
      Liu, Qun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.37",
    doi = "10.18653/v1/2020.emnlp-main.37",
    pages = "509--521",
    abstract = "Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.",
}

@inproceedings{
Lan2020ALBERT:,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@InProceedings{pmlr-v80-blanc18a,
  title = 	 {Adaptive Sampled Softmax with Kernel Based Sampling},
  author =       {Blanc, Guy and Rendle, Steffen},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {590--599},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/blanc18a/blanc18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/blanc18a.html},
  abstract = 	 {Softmax is the most commonly used output function for multiclass problems and is widely used in areas such as vision, natural language processing, and recommendation. A softmax model has linear costs in the number of classes which makes it too expensive for many real-world problems. A common approach to speed up training involves sampling only some of the classes at each training step. It is known that this method is biased and that the bias increases the more the sampling distribution deviates from the output distribution. Nevertheless, almost all recent work uses simple sampling distributions that require a large sample size to mitigate the bias. In this work, we propose a new class of kernel based sampling methods and develop an efficient sampling algorithm. Kernel based sampling adapts to the model as it is trained, thus resulting in low bias. It can also be easily applied to many models because it relies only on the model’s last hidden layer. We empirically study the trade-off of bias, sampling distribution and sample size and show that kernel based sampling results in low bias with few samples.}
}


@inproceedings{
bello2021lambdanetworks,
title={LambdaNetworks: Modeling long-range Interactions without Attention},
author={Irwan Bello},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=xTJEN-ggl1b}
}



@misc{dtrivgithub,
  author = {Casado, Mario Lezcano},
  title = {Optimization with orthogonal constraints and on general manifolds},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Lezcano/expRNN}}
}

@misc{ptbgithub,
  author = {Salesforce},
  title = {LSTM and QRNN Language Model Toolkit for PyTorch},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/salesforce/awd-lstm-lm}}
}




@inproceedings{kusupati2018fastgrnn,
  title={Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network},
  author={Kusupati, Aditya and Singh, Manish and Bhatia, Kush and Kumar, Ashish and Jain, Prateek and Varma, Manik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9017--9028},
  year={2018}
}

@inproceedings{chandar2019towards,
  title={Towards non-saturating recurrent units for modelling long-term dependencies},
  author={Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3280--3287},
  year={2019}
}

@article{van2018unreasonable,
  title={The unreasonable effectiveness of the forget gate},
  author={Van Der Westhuizen, Jos and Lasenby, Joan},
  journal={arXiv preprint arXiv:1804.04849},
  year={2018}
}


@article{merityRegOpt,
  title={{Regularizing and Optimizing LSTM Language Models}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}
@misc{mrtz,
  author={Hardt, Moritz },
  title = {Robustness versus acceleration},
  year={2014},
  howpublished = {\url{http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html}},
}
@article{merityAnalysis,
  title={{An Analysis of Neural Language Modeling at Multiple Scales}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1803.08240},
  year={2018}
}


@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}
@article{bjerrum2017molecular,
  title={Molecular generation with recurrent neural networks (RNNs)},
  author={Bjerrum, Esben Jannik and Threlfall, Richard},
  journal={arXiv preprint arXiv:1705.04612},
  year={2017}
}

@inproceedings{zhou2018sc2net,
  title={SC2Net: Sparse LSTMs for sparse coding},
  author={Zhou, Joey Tianyi and Di, Kai and Du, Jiawei and Peng, Xi and Yang, Hao and Pan, Sinno Jialin and Tsang, Ivor W and Liu, Yong and Qin, Zheng and Goh, Rick Siow Mong},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Eleventh Annual Conference of the International Speech Communication Association},
  year={2010}
}

@InProceedings{pmlr-v48-henaff16,
  title = 	 {Recurrent Orthogonal Networks and Long-Memory Tasks},
  author = 	 {Mikael Henaff and Arthur Szlam and Yann LeCun},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2034--2042},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/henaff16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/henaff16.html}
}



@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@article{song2021implicit,
      title={Implicit Kernel Attention}, 
      author={Kyungwoo Song and Yohan Jung and Dongjun Kim and Il-Chul Moon},
      year={2021},
      eprint={2006.06147},
      archivePrefix={arXiv},
      journal={arXiv preprint arXiv:2006.06147},
      primaryClass={cs.LG}
}

@article{mehta2020low,
      title={Low Rank Factorization for Compact Multi-Head Self-Attention}, 
      author={Sneha Mehta and Huzefa Rangwala and Naren Ramakrishnan},
      year={2020},
      eprint={1912.00835},
      archivePrefix={arXiv},
      journal={arXiv preprint arXiv:1912.00835},
      primaryClass={cs.CL}
}




@article{kotsias2019direct,
  title={Direct Steering of de novo Molecular Generation using Descriptor Conditional Recurrent Neural Networks (cRNNs)},
  author={Kotsias, Panagiotis-Christos and Ar{\'u}s-Pous, Josep and Chen, Hongming and Engkvist, Ola and Tyrchan, Christian and Bjerrum, Esben Jannik},
  year={2019},
  publisher={ChemRxiv}
}











@article{kag2019rnns,
  title={{RNNs} Evolving in Equilibrium: A Solution to the Vanishing and Exploding Gradients},
  author={Kag, Anil and Zhang, Ziming and Saligrama, Venkatesh},
  journal={arXiv preprint arXiv:1908.08574},
  year={2019}
}

@article{miller2018stable,
  title={Stable recurrent models},
  author={Miller, John and Hardt, Moritz},
  journal={arXiv preprint arXiv:1805.10369},
  year={2018}
}

@article{chen2019symplectic,
  title={Symplectic Recurrent Neural Networks},
  author={Chen, Zhengdao and Zhang, Jianyu and Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1909.13334},
  year={2019}
}

@article{chang2019antisymmetricrnn,
  title={AntisymmetricRNN: A dynamical system view on recurrent neural networks},
  author={Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H},
  journal={arXiv preprint arXiv:1902.09689},
  year={2019}
}

@inproceedings{casado2019trivializations,
  title={Trivializations for gradient-based optimization on manifolds},
  author={Casado, Mario Lezcano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9154--9164},
  year={2019}
}


@inproceedings{merity2018regularizing,
title={Regularizing and Optimizing {LSTM} Language Models},
author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyyGPP0TZ},
}

@inproceedings{cooijmans2016recurrent,
  title={Recurrent batch normalization},
  author={Tim Cooijmans and Nicolas Ballas and C{\'e}sar Laurent and {\c{C}}a{\u{g}}lar G{\"u}l{\c{c}}ehre and Aaron Courville},
  booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems},
  pages={2378--2386},
  year={2016}
}

@inproceedings{lezcano2019cheap,
  title={Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group},
  author={Lezcano-Casado, Mario and Mart{\'i}nez-Rubio, David},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={3794--3803},
  year={2019}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{he2021debertav3,
  title={Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  year={2021}
}

@InProceedings{pmlr-v80-parmar18a,
  title = 	 {Image Transformer},
  author =       {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4055--4064},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/parmar18a.html},
  abstract = 	 {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.}
}


@article{leethorp2021fnet,
      title={FNet: Mixing Tokens with Fourier Transforms}, 
      author={James Lee-Thorp and Joshua Ainslie and Ilya Eckstein and Santiago Ontanon},
      year={2021},
      eprint={2105.03824},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:2105.03824},
      primaryClass={cs.CL}
}


@article{xiong2021nystromformer,
  title={{Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention}},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}


@article{likhosherstov2020sublinear,
      title={Sub-Linear Memory: How to Make Performers SLiM}, 
      author={Valerii Likhosherstov and Krzysztof Choromanski and Jared Davis and Xingyou Song and Adrian Weller},
      year={2020},
      eprint={2012.11346},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:2012.11346},
      primaryClass={cs.LG}
}


@inproceedings{haoyietal-informer-2021,
  author    = {Haoyi Zhou and
               Shanghang Zhang and
               Jieqi Peng and
               Shuai Zhang and
               Jianxin Li and
               Hui Xiong and
               Wancai Zhang},
  title     = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  booktitle = {The Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021},
  pages     = {online},
  publisher = {{AAAI} Press},
  year      = {2021},
}


@inproceedings{DBLP:conf/nips/DarasKOD20,
  author={Giannis Daras and Nikita Kitaev and Augustus Odena and Alexandros G. Dimakis},
  title={SMYRF - Efficient Attention using Asymmetric Clustering},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/47d40767c7e9df50249ebfd9c7cfff77-Abstract.html},
  booktitle={NeurIPS}
}


@article{zaheer2021big,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
 journal={arXiv preprint arXiv:2007.14062},
      primaryClass={cs.LG}
}

@inproceedings{
schlag2021learning,
title={Learning Associative Inference Using Fast Weight Memory},
author={Imanol Schlag and Tsendsuren Munkhdalai and J{\"u}rgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=TuK6agbdt27}
}





@article{dhariwal2020jukebox,
  title={Jukebox: A Generative Model for Music},
  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2005.00341},
  year={2020}
}


@article{tay2020synthesizer,
      title={Synthesizer: Rethinking Self-Attention in Transformer Models}, 
      author={Yi Tay and Dara Bahri and Donald Metzler and Da-Cheng Juan and Zhe Zhao and Che Zheng},
      year={2020},
      eprint={2005.00743},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:2005.00743},
      primaryClass={cs.CL}
}


@article{subramanian2020multiscale,
      title={Multi-scale Transformer Language Models}, 
      author={Sandeep Subramanian and Ronan Collobert and Marc'Aurelio Ranzato and Y-Lan Boureau},
      year={2020},
      eprint={2005.00581},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:2005.00581},
      primaryClass={cs.CL}
}


@InProceedings{pmlr-v119-lioutas20a, title = {Time-aware Large Kernel Convolutions}, author = {Lioutas, Vasileios and Guo, Yuhong}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {6172--6183}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/lioutas20a/lioutas20a.pdf}, url = { http://proceedings.mlr.press/v119/lioutas20a.html }, abstract = {To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.} }


@article{transformerdiet2020,
   title={Transformer on a Diet},
   author={Chenguang Wang and Zihao Ye and Aston Zhang and Zheng Zhang and Alexander J. Smola},
   journal={ArXiv},
   year={2020},
   volume={abs/2002.06170}
}


@InProceedings{pmlr-v119-tay20a, title = {Sparse {S}inkhorn Attention}, author = {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {9438--9447}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/tay20a/tay20a.pdf}, url = { http://proceedings.mlr.press/v119/tay20a.html }, abstract = {We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.} }

@article{roy-etal-2021-efficient,
    title = "Efficient Content-Based Sparse Attention with Routing Transformers",
    author = "Roy, Aurko  and
      Saffar, Mohammad  and
      Vaswani, Ashish  and
      Grangier, David",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    url = "https://www.aclweb.org/anthology/2021.tacl-1.4",
    pages = "53--68",
}

@inproceedings{10.5555/2073796.2073829,
author = {Hofmann, Thomas},
title = {Probabilistic Latent Semantic Analysis},
year = {1999},
isbn = {1558606149},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
booktitle = {Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
pages = {289–296},
numpages = {8},
location = {Stockholm, Sweden},
series = {UAI'99}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}


@inproceedings{
Rae2020Compressive,
title={Compressive Transformers for Long-Range Sequence Modelling},
author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylKikSYDH}
}


@article{Ye2019BPTransformerML,
  title={BP-Transformer: Modelling Long-Range Context via Binary Partitioning},
  author={Zihao Ye and Qipeng Guo and Quan Gan and Xipeng Qiu and Zheng Zhang},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.04070}
}


@inproceedings{li19,
    author={Xia Li and Zhisheng Zhong and Jianlong Wu and Yibo Yang and Zhouchen Lin and Hong Liu},
    title={Expectation-Maximization Attention Networks for Semantic Segmentation},
    booktitle={International Conference on Computer Vision},   
    year={2019},   
}


@inproceedings{NEURIPS2019_9d8df73a,
 author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\textquotesingle Aurelio and Denoyer, Ludovic and Jegou, Herve},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Large Memory Layers with Product Keys},
 url = {https://proceedings.neurips.cc/paper/2019/file/9d8df73a3cfbf3c5b47bc9b50f214aff-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{Joutard_2019,
   title={Permutohedral Attention Module for Efficient Non-local Neural Networks},
   ISBN={9783030322267},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-32226-7_44},
   DOI={10.1007/978-3-030-32226-7_44},
   journal={Medical Image Computing and Computer Assisted Intervention – MICCAI 2019},
   publisher={Springer International Publishing},
   author={Joutard, Samuel and Dorent, Reuben and Isaac, Amanda and Ourselin, Sebastien and Vercauteren, Tom and Modat, Marc},
   year={2019},
   pages={393–401}
}


@article{huang2019interlaced,
      title={Interlaced Sparse Self-Attention for Semantic Segmentation}, 
      author={Lang Huang and Yuhui Yuan and Jianyuan Guo and Chao Zhang and Xilin Chen and Jingdong Wang},
      year={2019},
      eprint={1907.12273},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:1907.12273},
      primaryClass={cs.CV}
}

@article{calian2019scram,
      title={SCRAM: Spatially Coherent Randomized Attention Maps}, 
      author={Dan A. Calian and Peter Roelants and Jacques Cali and Ben Carr and Krishna Dubba and John E. Reid and Dell Zhang},
      year={2019},
      eprint={1905.10308},
      archivePrefix={arXiv},
journal={arXiv preprint arXiv:1905.10308},
      primaryClass={cs.LG}
}


@INPROCEEDINGS{9022134,
  author={Cao, Yue and Xu, Jiarui and Lin, Stephen and Wei, Fangyun and Hu, Han},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)}, 
  title={GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond}, 
  year={2019},
  volume={},
  number={},
  pages={1971-1980},
  doi={10.1109/ICCVW.2019.00246}}


@InProceedings{pmlr-v97-lee19d, title = {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks}, author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {3744--3753}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf}, url = { http://proceedings.mlr.press/v97/lee19d.html }, abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.} }

@InProceedings{Woo_2018_ECCV,
author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
title = {CBAM: Convolutional Block Attention Module},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}


@inproceedings{
j.2018generating,
title={Generating Wikipedia by Summarizing Long Sequences},
author={Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hyg0vbWC-},
}

@inproceedings{tenney-etal-2019-bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@inproceedings{vig-belinkov-2019-analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4808",
    doi = "10.18653/v1/W19-4808",
    pages = "63--76",
    abstract = "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.",
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@inproceedings{hewitt-liang-2019-designing,
    title = "Designing and Interpreting Probes with Control Tasks",
    author = "Hewitt, John  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1275",
    doi = "10.18653/v1/D19-1275",
    pages = "2733--2743",
    abstract = "Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe{'}s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.",
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}



@inproceedings{you-etal-2020-hard,
    title = "Hard-Coded {G}aussian Attention for Neural Machine Translation",
    author = "You, Weiqiu  and
      Sun, Simeng  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.687",
    pages = "7689--7700",
    abstract = "Recent work has questioned the importance of the Transformer{'}s multi-headed attention for achieving high translation quality. We push further in this direction by developing a {``}hard-coded{''} attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.",
}

@inproceedings{raganato-etal-2020-fixed,
    title = "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Scherrer, Yves  and
      Tiedemann, J{\"o}rg},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.49",
    doi = "10.18653/v1/2020.findings-emnlp.49",
    pages = "556--568",
    abstract = "Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed {--} non-learnable {--} attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",
}

@inproceedings{ainslie-etal-2020-etc,
    title = "{ETC}: Encoding Long and Structured Inputs in Transformers",
    author = "Ainslie, Joshua  and
      Ontanon, Santiago  and
      Alberti, Chris  and
      Cvicek, Vaclav  and
      Fisher, Zachary  and
      Pham, Philip  and
      Ravula, Anirudh  and
      Sanghai, Sumit  and
      Wang, Qifan  and
      Yang, Li",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.19",
    doi = "10.18653/v1/2020.emnlp-main.19",
    pages = "268--284",
    abstract = "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, {``}Extended Transformer Construction{''} (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a {``}Contrastive Predictive Coding{''} (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
}







@InProceedings{touvron2020deit,
  title = 	 {Training data-efficient image transformers distillation through attention},
  author =       {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10347--10357},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/touvron21a.html}
}

@article{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      journal={arXiv preprint arXiv:1910.01108},
      primaryClass={cs.CL}
}

@inproceedings{47866,
title	= {Character-Level Language Modeling with Deeper Self-Attention},
author	= {Rami Al-Rfou and DK Choe and Noah Constant and Mandy Guo and Llion Jones},
year	= {2019},
URL	= {https://arxiv.org/abs/1808.04444},
booktitle	= {Thirty-Third AAAI Conference on Artificial Intelligence}
}

@inproceedings{
baevski2018adaptive,
title={Adaptive Input Representations for Neural Language Modeling},
author={Alexei Baevski and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByxZX20qFQ},
}

@article{Radford2019,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/shanest/Documents/Library/Radford et al/Unknown/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf:pdf},
keywords = {model},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://openai.com/blog/better-language-models/},
year = {2019}
}

@inproceedings{parikh-etal-2016-decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}


@article{DBLP:journals/corr/LinFSYXZB17,
  author    = {Zhouhan Lin and
               Minwei Feng and
               C{\'{\i}}cero Nogueira dos Santos and
               Mo Yu and
               Bing Xiang and
               Bowen Zhou and
               Yoshua Bengio},
  title     = {A Structured Self-attentive Sentence Embedding},
  journal   = {CoRR},
  volume    = {abs/1703.03130},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.03130},
  archivePrefix = {arXiv},
  eprint    = {1703.03130},
  timestamp = {Mon, 13 Aug 2018 16:46:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LinFSYXZB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bebendorf.M;Ostrowski.J2009,
  title = {Parallel Hierarchical Matrix Preconditioners for the Curl-Curl Operator},
  author = {Bebendorf, Mario and Ostrowski, Joerg},
  year = {2009},
  volume = {27},
  pages = {624--641},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Bebendorf, Ostrowski/J. Comput. Math/Bebendorf, Ostrowski - 2009 - Parallel hierarchical matrix preconditioners for the curl-curl operator.pdf},
  journal = {J. Comput. Math.},
  keywords = {computational electromagnetism,hierarchical matrices,par-,preconditioners},
  note = {\textbf{From Duplicate 2 ( }                \textbf{          }\textbf{\emph{Parallel hierarchical matrix preconditioners for the curl-curl operator}}\textbf{        }                \textbf{ - Bebendorf, Mario; Ostrowski, Joerg )
        }        
        \textbf{From Duplicate 1 ( }                \textbf{          }\textbf{\emph{PARALLEL HIERARCHICAL MATRIX PRECONDITIONERS}}\textbf{        }                \textbf{ - Bebendorf, Mario; Ostrowski, Joerg )
        }        
        
        
        
        \\},
  number = {5}
}

@article{Bebendorf.M2007,
  title = {Why {{Finite Element Discretizations Can Be Factored}} by {{Triangular Hierarchical Matrices}}},
  author = {Bebendorf, Mario},
  year = {2007},
  volume = {45},
  pages = {1472--1494},
  publisher = {{SIAM}},
  doi = {10.1137/060669747},
  file = {/Volumes/GoogleDrive/My Drive/biblib/Papers/Bebendorf/SIAM J. Numer. Anal/Bebendorf - 2007 - Why Finite Element Discretizations Can Be Factored by Triangular Hierarchical Matrices.pdf;/Volumes/GoogleDrive/My Drive/biblib/Papers/Bebendorf/SIAM J. Numer. Anal/Bebendorf - 2007 - Why Finite Element Discretizations Can Be Factored by Triangular Hierarchical Matrices.pdf;/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Bebendorf/SIAM J. Numer. Anal/Bebendorf - 2007 - Why Finite Element Discretizations Can Be Factored by Triangular Hierarchical Matrices.pdf;/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Bebendorf/SIAM J. Numer. Anal/Bebendorf - 2007 - Why Finite Element Discretizations Can Be Factored by Triangular Hierarchical Matrices.pdf},
  journal = {SIAM J. Numer. Anal.},
  keywords = {approximate LU decomposition,approximate LU decomposition; fast direct solution,fast direct solution},
  number = {4}
}

@article{Bebendorf2003,
  title = {Existence of {{H}} -Matrix Approximants to the Inverse {{FE}}-Matrix of Elliptic Operators with {{L}} {$\infty$} -Coefficients},
  author = {Bebendorf, Mario and Hackbusch, Wolfgang},
  year = {2003},
  pages = {1--28},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Papers/Bebendorf, Hackbusch/Numerische Mathematik/Bebendorf, Hackbusch - 2003 - Existence of H -matrix approximants to the inverse FE-matrix of elliptic operators with L ∞ -coefficient.pdf},
  journal = {Numerische Mathematik}
}

@article{Bebendorf2004,
  title = {Efficient Inversion of the {{Galerkin}} Matrix of General Second-Order Elliptic Operators with Nonsmooth Coefficients},
  author = {Bebendorf, Mario},
  year = {2004},
  month = sep,
  volume = {74},
  pages = {1179--1200},
  issn = {0025-5718},
  doi = {10.1090/S0025-5718-04-01716-8},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Bebendorf/Mathematics of Computation/Bebendorf - 2004 - Efficient inversion of the Galerkin matrix of general second-order elliptic operators with nonsmooth coefficients.pdf},
  journal = {Mathematics of Computation},
  number = {251}
}

@article{Borm2003,
  title = {Introduction to Hierarchical Matrices with Applications},
  author = {B{\"o}rm, Steffen and Grasedyck, Lars and Hackbusch, Wolfgang},
  year = {2003},
  month = may,
  volume = {27},
  pages = {405--422},
  issn = {09557997},
  doi = {10.1016/S0955-7997(02)00152-2},
  file = {/Volumes/GoogleDrive/My Drive/biblib/Papers/Börm, Grasedyck, Hackbusch/Engineering Analysis with Boundary Elements/Börm, Grasedyck, Hackbusch - 2003 - Introduction to hierarchical matrices with applications.pdf;/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Börm, Grasedyck, Hackbusch/Engineering Analysis with Boundary Elements/Börm, Grasedyck, Hackbusch - 2003 - Introduction to hierarchical matrices with applications.pdf},
  journal = {Engineering Analysis with Boundary Elements},
  keywords = {65f05,65f30,65f50,65n50},
  number = {5}
}

@article{Borm2005,
  title = {Hierarchical {{Matrices}}},
  author = {Borm, Steffen and Grasedyck, Lars and Hackbusch, Wolfgang},
  year = {2005},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Borm, Grasedyck, Hackbusch/Tutorial/Borm, Grasedyck, Hackbusch - 2005 - Hierarchical Matrices.pdf},
  journal = {Tutorial}
}

@article{Borne2008,
  title = {Hierarchical Matrix Preconditioners for the {{Oseen}} Equations},
  author = {Borne, Sabine Le},
  year = {2008},
  volume = {38505},
  pages = {1--10},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Borne/Computing and Visualization in Science/Borne - 2008 - Hierarchical matrix preconditioners for the Oseen equations.pdf},
  journal = {Computing and Visualization in Science},
  keywords = {65f05,65f30,65f50,ams subject classification,data-sparse approximation,equations,hierarchical matrices,regularization,schur complement,stokes}
}

@article{Corona2015,
  title = {An {{O}} ({{N}}) Direct Solver for Integral Equations on the Plane},
  author = {Corona, Eduardo and Martinsson, Per-gunnar and Zorin, Denis},
  year = {2015},
  volume = {38},
  pages = {284--317},
  publisher = {{Elsevier Inc.}},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2014.04.002},
  abstract = {An efficient direct solver for volume integral equations with O(N)complexity for a broad range of problems is presented. The solver relies on hierarchical compression of the discretized integral operator, and exploits that off-diagonal blocks of certain dense matrices have numerically low rank. Technically, the solver is inspired by previously developed direct solvers for integral equations based on ``recursive skeletonization'' and ``Hierarchically Semi-Separable'' (HSS) matrices, but it improves on the asymptotic complexity of existing solvers by incorporating an additional level of compression. The resulting solver has optimal O(N)complexity for all stages of the computation, as demonstrated by both theoretical analysis and numerical examples. The computational examples further display good practical performance in terms of both speed and memory usage. In particular, it is demonstrated that even problems involving 107 unknowns can be solved to precision 10-10 using a simple Matlab implementation of the algorithm executed on a single core},
  file = {/Volumes/GoogleDrive/My Drive/biblib/Papers/Corona, Martinsson, Zorin/Applied and Computational Harmonic Analysis/Corona, Martinsson, Zorin - 2015 - An O (N) direct solver for integral equations on the plane.pdf},
  journal = {Applied and Computational Harmonic Analysis},
  number = {2}
}

@article{Engquist2011,
  title = {Sweeping {{Preconditioner}} for the {{Helmholtz Equation}} : {{Hierarchical Matrix Representation}}},
  author = {Engquist, Bj{\"o}rn},
  year = {2011},
  volume = {LXIV},
  pages = {697--735},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Engquist/Unknown/Engquist - 2011 - Sweeping Preconditioner for the Helmholtz Equation Hierarchical Matrix Representation.pdf}
}

@article{Faustmann2013,
  title = {H -Matrix Approximability of the Inverses of {{FEM}} Matrices},
  author = {Faustmann, Markus and Aug, N A},
  year = {2013},
  pages = {1--23},
  archiveprefix = {arXiv},
  eprint = {1308.0499v1},
  eprinttype = {arxiv},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Faustmann, Aug/Unknown/Faustmann, Aug - 2013 - H -matrix approximability of the inverses of FEM matrices.pdf}
}

@article{Faustmann2015,
  title = {H -Matrix Approximability of the Inverses of {{FEM}} Matrices},
  author = {Faustmann, Markus and Markus, Jens},
  year = {2015},
  volume = {131},
  pages = {615--642},
  publisher = {{Springer Berlin Heidelberg}},
  issn = {0029-599X},
  doi = {10.1007/s00211-015-0706-9},
  file = {/Volumes/GoogleDrive/My Drive/biblib/Papers/Faustmann, Markus/Numerische Mathematik/Faustmann, Markus - 2015 - H -matrix approximability of the inverses of FEM matrices.pdf},
  isbn = {0021101507069},
  journal = {Numerische Mathematik},
  keywords = {★,65F05,65F30,65F50,65N30}
}

@article{Graham.I;Grasedyck.L;Hackbusch.W;Sauter.S2008,
  title = {Optimal {{Panel}}-{{Clustering}} in the {{Presence}} of {{Anisotropic Mesh Refinement}}},
  author = {Graham, I G and Grasedyck, L and Hackbusch, W and Sauter, S A},
  year = {2008},
  volume = {46},
  pages = {517--543},
  publisher = {{SIAM}},
  doi = {10.1137/060677987},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Graham et al/SIAM J. Numer. Anal/Graham et al. - 2008 - Optimal Panel-Clustering in the Presence of Anisotropic Mesh Refinement.pdf},
  journal = {SIAM J. Numer. Anal.},
  keywords = {060677987,1,10,1137,65n12,65n22,65n38,65r20,ams subject classifications,anisotropic m,anisotropic meshes,boundary elements,doi,for,introduction,is a well-known method,matrix compression,panel-clustering,the boundary integral method},
  number = {1}
}

@article{Grasedyck.L;Hackbusch.W2003,
  title = {Construction and {{Arithmetics}} of \{\textbackslash mathcal \vphantom\}{{H}}\vphantom\{\}-Matrices},
  author = {Grasedyck, Lars and Hackbusch, Wolfgang},
  year = {2003},
  volume = {70},
  pages = {295--334},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Grasedyck, Hackbusch/Computing/Grasedyck, Hackbusch - 2003 - Construction and Arithmetics of mathcal H -matrices.pdf},
  journal = {Computing}
}

@article{Grasedyck2003,
  title = {Solution of {{Large Scale Algebraic Matrix Riccati Equations}} by {{Use}} of {{Hierarchical Matrices}}},
  author = {Grasedyck, L. and Hackbusch, W. and Khoromskij, B. N.},
  year = {2003},
  month = apr,
  volume = {70},
  pages = {121--165},
  issn = {0010-485X},
  doi = {10.1007/s00607-002-1470-0},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Grasedyck, Hackbusch, Khoromskij/Computing/Grasedyck, Hackbusch, Khoromskij - 2003 - Solution of Large Scale Algebraic Matrix Riccati Equations by Use of Hierarchical Matrices.pdf},
  journal = {Computing},
  keywords = {control problems,data-sparse approximations,fast,formatted matrix operations,hierarchical matrices,lyapunov equations,riccati equations,solvers},
  number = {2}
}



@article{Hackbusch.W1999,
author = {Hackbusch, W.},
title = {A Sparse Matrix Arithmetic Based on H-Matrices. Part I: Introduction to H-Matrices},
year = {1999},
issue_date = {1999},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {2},
issn = {0010-485X},
url = {https://doi.org/10.1007/s006070050015},
doi = {10.1007/s006070050015},
journal = {Computing},
month = may,
pages = {89–108},
numpages = {20},
keywords = {matrix inversion, hierarchical block partitioning, hierarchical matrices, sparse matrices}
}



@article{Hackbusch2008,
  title = {Approximate Iterations for Structured Matrices},
  author = {Hackbusch, Wolfgang and Khoromskij, Boris N. and Tyrtyshnikov, Eugene E.},
  year = {2008},
  month = feb,
  volume = {109},
  pages = {365--383},
  issn = {0029-599X},
  doi = {10.1007/s00211-008-0143-0},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Hackbusch, Khoromskij, Tyrtyshnikov/Numerische Mathematik/Hackbusch, Khoromskij, Tyrtyshnikov - 2008 - Approximate iterations for structured matrices.pdf},
  journal = {Numerische Mathematik},
  number = {3}
}

@article{Lin2011,
  title = {Fast Construction of Hierarchical Matrix Representation from Matrix\textendash Vector Multiplication},
  author = {Lin, Lin and Lu, Jianfeng and Ying, Lexing},
  year = {2011},
  month = may,
  volume = {230},
  pages = {4071--4087},
  publisher = {{Elsevier Inc.}},
  issn = {00219991},
  doi = {10.1016/j.jcp.2011.02.033},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Lin, Lu, Ying/Journal of Computational Physics/Lin, Lu, Ying - 2011 - Fast construction of hierarchical matrix representation from matrix–vector multiplication.pdf},
  journal = {Journal of Computational Physics},
  number = {10}
}

@article{Steffen,
  title = {Hierarchical {{Matrix}}. {{Short Course}} on {{Boundary Element Methods}}},
  author = {Steffen, B},
  file = {/Volumes/LongBackupDrive/GoogleDrive/biblib/Papers/Steffen/Presentation/Steffen - Unknown - Hierarchical Matrix. Short Course on Boundary Element Methods.pdf},
  journal = {Presentation}
}



@article{vogelSuperfastDivideandConquerMethod2016,
  author    = {James Vogel and
               Jianlin Xia and
               Stephen Cauley and
               Venkataramanan Balakrishnan},
  title     = {Superfast Divide-and-Conquer Method and Perturbation Analysis for
               Structured Eigenvalue Solutions},
  journal   = {{SIAM} J. Sci. Comput.},
  volume    = {38},
  number    = {3},
  year      = {2016},
  url       = {https://doi.org/10.1137/15M1018812},
  doi       = {10.1137/15M1018812},
  timestamp = {Thu, 30 Jul 2020 14:28:49 +0200},
  biburl    = {https://dblp.org/rec/journals/siamsc/VogelXCB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xiaFastAlgorithmsHierarchically2010,
  author    = {Jianlin Xia and
               Shivkumar Chandrasekaran and
               Ming Gu and
               Xiaoye S. Li},
  title     = {Fast algorithms for hierarchically semiseparable matrices},
  journal   = {Numer. Linear Algebra Appl.},
  volume    = {17},
  number    = {6},
  pages     = {953--976},
  year      = {2010},
  url       = {https://doi.org/10.1002/nla.691},
  doi       = {10.1002/nla.691},
  timestamp = {Mon, 11 May 2020 22:58:14 +0200},
  biburl    = {https://dblp.org/rec/journals/nla/XiaCGL10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Xia.J;Chandrasekaran.S;Gu.M;Li.X2009,
  author    = {Jianlin Xia and
               Shivkumar Chandrasekaran and
               Ming Gu and
               Xiaoye S. Li},
  title     = {Superfast Multifrontal Method for Large Structured Linear Systems
               of Equations},
  journal   = {{SIAM} J. Matrix Anal. Appl.},
  volume    = {31},
  number    = {3},
  pages     = {1382--1411},
  year      = {2009},
  url       = {https://doi.org/10.1137/09074543X},
  doi       = {10.1137/09074543X},
  timestamp = {Tue, 26 May 2020 08:51:53 +0200},
  biburl    = {https://dblp.org/rec/journals/siammax/XiaCGL09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{nguyen2018bayesian,
  title={A {B}ayesian perspective of convolutional neural networks through a deconvolutional generative model},
  author={Nguyen, Tan and Ho, Nhat and Patel, Ankit and Anandkumar, Anima and Jordan, Michael I and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:1811.02657},
  year={2018}
}

@article{patel2016probabilistic,
  title={A probabilistic framework for deep learning},
  author={Patel, Ankit B and Nguyen, Minh T and Baraniuk, Richard},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={2558--2566},
  year={2016}
}

@inproceedings{cho2020meantime,
  title={MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation},
  author={Cho, Sung Min and Park, Eunhyeok and Yoo, Sungjoo},
  booktitle={Fourteenth ACM Conference on Recommender Systems},
  pages={515--520},
  year={2020}
}

@inproceedings{guo2019gaussian,
  title={Gaussian transformer: a lightweight approach for natural language inference},
  author={Guo, Maosheng and Zhang, Yu and Liu, Ting},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6489--6496},
  year={2019}
}

@inproceedings{jiang2020transformer,
  title={Transformer vae: A hierarchical model for structure-aware and interpretable music representation learning},
  author={Jiang, Junyan and Xia, Gus G and Carlton, Dave B and Anderson, Chris N and Miyakawa, Ryan H},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={516--520},
  year={2020},
  organization={IEEE}
}

@article{dalvi2020analyzing,
  title={Analyzing redundancy in pretrained transformer models},
  author={Dalvi, Fahim and Sajjad, Hassan and Durrani, Nadir and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2004.04010},
  year={2020}
}

@article{durrani2020analyzing,
  title={Analyzing individual neurons in pre-trained language models},
  author={Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2010.02695},
  year={2020}
}

@inproceedings{
mu2018allbutthetop,
title={All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
author={Jiaqi Mu and Pramod Viswanath},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkuGJ3kCb},
}

@article{ethayarajh2019contextual,
  title={How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings},
  author={Ethayarajh, Kawin},
  journal={arXiv preprint arXiv:1909.00512},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@article{sajjad2020poor,
  title={Poor Man's BERT: Smaller and Faster Transformer Models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={arXiv e-prints},
  pages={arXiv--2004},
  year={2020}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{hanson1990stochastic,
  title={A stochastic version of the delta rule},
  author={Hanson, Stephen Jos{\'e}},
  journal={Physica D: Nonlinear Phenomena},
  volume={42},
  number={1-3},
  pages={265--272},
  year={1990},
  publisher={Elsevier}
}

@misc{touvron2021training,
      title={Training data-efficient image transformers & distillation through attention}, 
      author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
      year={2021},
      eprint={2012.12877},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@InProceedings{pmlr-v139-touvron21a,
  title = 	 {Training data-efficient image transformers &amp; distillation through attention},
  author =       {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10347--10357},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/touvron21a.html},
  abstract = 	 {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}

@article{DBLP:journals/corr/abs-2103-14030,
  author    = {Ze Liu and
               Yutong Lin and
               Yue Cao and
               Han Hu and
               Yixuan Wei and
               Zheng Zhang and
               Stephen Lin and
               Baining Guo},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  journal   = {CoRR},
  volume    = {abs/2103.14030},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.14030},
  eprinttype = {arXiv},
  eprint    = {2103.14030},
  timestamp = {Thu, 08 Apr 2021 07:53:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@article{DBLP:journals/corr/abs-2012-12877,
  author    = {Hugo Touvron and
               Matthieu Cord and
               Matthijs Douze and
               Francisco Massa and
               Alexandre Sablayrolles and
               Herv{\'{e}} J{\'{e}}gou},
  title     = {Training data-efficient image transformers {\&} distillation through
               attention},
  journal   = {CoRR},
  volume    = {abs/2012.12877},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.12877},
  eprinttype = {arXiv},
  eprint    = {2012.12877},
  timestamp = {Tue, 05 Jan 2021 16:02:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-12877.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{bagnall2018uea,
  title={The UEA multivariate time series classification archive, 2018},
  author={Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
  journal={arXiv preprint arXiv:1811.00075},
  year={2018}
}

@inproceedings{wu2022flowformer,
  title={Flowformer: Linearizing Transformers with Conservation Flows},
  author={Haixu Wu and Jialong Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@inproceedings{zerveas2021transformer,
  title={A transformer-based framework for multivariate time series representation learning},
  author={Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2114--2124},
  year={2021}
}


@article{guo2021pct,
  title={Pct: Point cloud transformer},
  author={Guo, Meng-Hao and Cai, Jun-Xiong and Liu, Zheng-Ning and Mu, Tai-Jiang and Martin, Ralph R and Hu, Shi-Min},
  journal={Computational Visual Media},
  volume={7},
  number={2},
  pages={187--199},
  year={2021},
  publisher={Springer}
}

@inproceedings{zhao2021point,
  title={Point transformer},
  author={Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip HS and Koltun, Vladlen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={16259--16268},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6824--6835},
  year={2021}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}


@inproceedings{nguyen2022transformer,
  title={Fourier Former: Transformer Meets Generalized Fourier Integral Theorem},
  author={Nguyen, Tan and Pham, Minh and Nguyen, Tam and Nguyen, Khai and Osher, Stanley J and Ho, Nhat},
  booktitle={Advances in neural information processing systems},
  year={2022}
}

@article{cao2021choose,
  title={Choose a transformer: Fourier or galerkin},
  author={Cao, Shuhao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@book{reddy2004introduction,
  title={An introduction to the finite element method},
  author={Reddy, JN},
  volume={1221},
  year={2004},
  publisher={McGraw-Hill New York}
}

@article{Tan2020MonashUU,
  title={Monash University, UEA, UCR Time Series Regression Archive},
  author={Chang Wei Tan and C. Bergmeir and François Petitjean and Geoffrey I. Webb},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.10996}
}

@inproceedings{NEURIPS2018_ec895663,
 author = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning long-range spatial dependencies with horizontal gated recurrent units},
 url = {https://proceedings.neurips.cc/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf},
 volume = {31},
 year = {2018}
}
@conference{005357537ef34c158137a70c5df3799b,
title = "INFOBERT: IMPROVING ROBUSTNESS OF LANGUAGE MODELS FROM AN INFORMATION THEORETIC PERSPECTIVE",
abstract = "Large-scale pre-trained language models such as BERT and RoBERTa have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) an Anchored Feature regularizer, which increases the mutual information between local stable features and global features. We provide a principled way to theoretically analyze and improve the robustness of language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.",
author = "Boxin Wang and Shuohang Wang and Yu Cheng and Zhe Gan and Ruoxi Jia and Bo Li and Jingjing Liu",
note = "Publisher Copyright: {\textcopyright} 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.; 9th International Conference on Learning Representations, ICLR 2021 ; Conference date: 03-05-2021 Through 07-05-2021",
year = "2021",
language = "English (US)",
}
@inproceedings{peyrard-etal-2022-invariant,
    title = "Invariant Language Modeling",
    author = "Peyrard, Maxime  and
      Ghotra, Sarvjeet  and
      Josifoski, Martin  and
      Agarwal, Vidhan  and
      Patra, Barun  and
      Carignan, Dean  and
      Kiciman, Emre  and
      Tiwary, Saurabh  and
      West, Robert",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.387",
    doi = "10.18653/v1/2022.emnlp-main.387",
    pages = "5728--5743",
    abstract = "Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases. Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose invariant language modeling, a framework for learning invariant representations that generalize better across multiple environments. In particular, we adapt a game-theoretic implementation of IRM (IRM-games) to language models, where the invariance emerges from a specific training schedule in which all the environments compete to optimize their own environment-specific loss by updating subsets of the model in a round-robin fashion. We focused on controlled experiments to precisely demonstrate the ability of our method to (i) remove structured noise, (ii) ignore specific spurious correlations without affecting global performance, and (iii) achieve better out-of-domain generalization. These benefits come with a negligible computational overhead compared to standard training, do not require changing the local loss, and can be applied to any language model. We believe this framework is promising to help mitigate spurious correlations and biases in language models.",
}
@inproceedings{yuan2023you,
  title={You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks?},
  author={Yuan, Zenghui and Zhou, Pan and Zou, Kai and Cheng, Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24605--24615},
  year={2023}
}
@inproceedings{paul2022vision,
  title={Vision transformers are robust learners},
  author={Paul, Sayak and Chen, Pin-Yu},
  booktitle={Proceedings of the AAAI conference on Artificial Intelligence},
  volume={36},
  number={2},
  pages={2071--2081},
  year={2022}
}
@inproceedings{bhojanapalli2021understanding,
  title={Understanding robustness of transformers for image classification},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10231--10241},
  year={2021}
}
@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}
@inproceedings{mao2022towards,
  title={Towards robust vision transformer},
  author={Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Duan, Ranjie and Ye, Shaokai and He, Yuan and Xue, Hui},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={12042--12051},
  year={2022}
}
@inproceedings{zhou2022understanding,
  title={Understanding the robustness in vision transformers},
  author={Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Animashree and Feng, Jiashi and Alvarez, Jose M},
  booktitle={International Conference on Machine Learning},
  pages={27378--27394},
  year={2022},
  organization={PMLR}
}
@inproceedings{mahmood2021robustness,
  title={On the robustness of vision transformers to adversarial examples},
  author={Mahmood, Kaleel and Mahmood, Rigel and Van Dijk, Marten},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7838--7847},
  year={2021}
}
@inproceedings{jin2020bert,
  title={Is bert really robust? a strong baseline for natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8018--8025},
  year={2020}
}
@article{zang2019word,
  title={Word-level textual adversarial attacking as combinatorial optimization},
  author={Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
  journal={arXiv preprint arXiv:1910.12196},
  year={2019}
}

@book{strang2006linear,
  added-at = {2014-09-12T13:40:38.000+0200},
  address = {Belmont, CA},
  author = {Strang, Gilbert},
  biburl = {https://www.bibsonomy.org/bibtex/2928ae987a1afe04daac3c76587856389/alex_ruff},
  description = {Linear Algebra and Its Applications, 4th Edition: Gilbert Strang: 9780030105678: Amazon.com: Books},
  interhash = {89d8f10be9cdd770dd9639ac4d469e97},
  intrahash = {928ae987a1afe04daac3c76587856389},
  isbn = {0030105676 9780030105678 0534422004 9780534422004},
  keywords = {book math to_READ},
  publisher = {Thomson, Brooks/Cole},
  refid = {61231077},
  timestamp = {2014-09-12T13:40:38.000+0200},
  title = {Linear algebra and its applications},
  url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
  year = 2006
}
@article{stanbregman,
author = {Yin, Wotao and Osher, Stanley and Goldfarb, Donald and Darbon, Jerome},
year = {2008},
month = {01},
pages = {},
title = {Bregman Iterative Algorithms for l(1)-Minimization with Applications to Compressed Sensing},
volume = {1},
journal = {Siam Journal on Imaging Sciences - SIAM J IMAGING SCI},
doi = {10.1137/070703983}
}

@article{zhangnonlocalbregman,
author = {Zhang, Xiaoqun and Burger, Martin and Bresson, Xavier and Osher, Stanley},
title = {Bregmanized Nonlocal Regularization for Deconvolution and Sparse Reconstruction},
journal = {SIAM Journal on Imaging Sciences},
volume = {3},
number = {3},
pages = {253-276},
year = {2010},
doi = {10.1137/090746379},

URL = { 
    
        https://doi.org/10.1137/090746379
    
    

},
eprint = { 
    
        https://doi.org/10.1137/090746379
    
    

}
,
    abstract = { Bregman methods introduced in [S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin, Multiscale Model. Simul., 4 (2005), pp. 460–489] to image processing are demonstrated to be an efficient optimization method for solving sparse reconstruction with convex functionals, such as the \$\ell^1\$ norm and total variation [W. Yin, S. Osher, D. Goldfarb, and J. Darbon, SIAM J. Imaging Sci., 1 (2008), pp. 143–168; T. Goldstein and S. Osher, SIAM J. Imaging Sci., 2 (2009), pp. 323–343]. In particular, the efficiency of this method relies on the performance of inner solvers for the resulting subproblems. In this paper, we propose a general algorithm framework for inverse problem regularization with a single forward-backward operator splitting step [P. L. Combettes and V. R. Wajs, Multiscale Model. Simul., 4 (2005), pp. 1168–1200], which is used to solve the subproblems of the Bregman iteration. We prove that the proposed algorithm, namely, Bregmanized operator splitting (BOS), converges without fully solving the subproblems. Furthermore, we apply the BOS algorithm and a preconditioned one for solving inverse problems with nonlocal functionals. Our numerical results on deconvolution and compressive sensing illustrate the performance of nonlocal total variation regularization under the proposed algorithm framework, compared to other regularization techniques such as the standard total variation method and the wavelet-based regularization method. }
}

@inproceedings{dong2020benchmarking,
  title={Benchmarking adversarial robustness on image classification},
  author={Dong, Yinpeng and Fu, Qi-An and Yang, Xiao and Pang, Tianyu and Su, Hang and Xiao, Zihao and Zhu, Jun},
  booktitle={proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={321--331},
  year={2020}
}
@article{tramer2019adversarial,
  title={Adversarial training and robustness for multiple perturbations},
  author={Tramer, Florian and Boneh, Dan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{MORACA2007666,
title = {Upper bounds for the infinity norm of the inverse of SDD and S-SDD matrices},
journal = {Journal of Computational and Applied Mathematics},
volume = {206},
number = {2},
pages = {666-678},
year = {2007},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2006.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377042706005139},
author = {Nenad Morača},
keywords = {Infinity norm of the inverse, Varah bound, Non-singularity results, Scaling, Strict diagonal dominance},
abstract = {An easily calculable upper bound for the infinity norm of the inverse of S-strictly diagonally dominant (S-SDD) matrices, introduced by Gao and Wang in 1992, is obtained in this paper. Applying that bound to SDD matrices, we improve the Varah bound. We also introduce an iterative procedure for obtaining sharper bounds, accompanied with some open questions.}
}

@article{13ab5b5e-0237-33fb-a7a8-6f6e4e0d4e0f,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2280232},
 abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
 author = {Nicholas Metropolis and S. Ulam},
 journal = {Journal of the American Statistical Association},
 number = {247},
 pages = {335--341},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {The Monte Carlo Method},
 urldate = {2024-02-01},
 volume = {44},
 year = {1949}
}
@article{0826aabe-2dc2-393a-bbf2-a98a36b6bed5,
 ISSN = {00255572},
 URL = {http://www.jstor.org/stable/3620776},
 author = {John R. Silvester},
 journal = {The Mathematical Gazette},
 number = {501},
 pages = {460--467},
 publisher = {Mathematical Association},
 title = {Determinants of Block Matrices},
 urldate = {2024-02-01},
 volume = {84},
 year = {2000}
}
@inproceedings{wang-bansal-2018-robust,
    title = "Robust Machine Comprehension Models via Adversarial Training",
    author = "Wang, Yicheng  and
      Bansal, Mohit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2091",
    doi = "10.18653/v1/N18-2091",
    pages = "575--581",
    abstract = "It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50{\%} decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent{'}s semantic perturbations (e.g., antonyms), we jointly improve the model{'}s semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art model significantly more robust, achieving a 36.5{\%} increase in F1 score under many different types of adversarial evaluation while maintaining performance on the regular SQuAD task.",
}

@InProceedings{pmlr-v80-uesato18a,
  title = 	 {Adversarial Risk and the Dangers of Evaluating Against Weak Attacks},
  author =       {Uesato, Jonathan and O'Donoghue, Brendan and Kohli, Pushmeet and van den Oord, Aaron},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5025--5034},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/uesato18a/uesato18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/uesato18a.html},
  abstract = 	 {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate <em>adversarial risk</em> as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as <em>obscurity to an adversary</em>, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.}
}

@inproceedings{NEURIPS2019_5d4ae76f,
 author = {Tramer, Florian and Boneh, Dan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adversarial Training and Robustness for Multiple Perturbations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf},
 volume = {32},
 year = {2019}
}

