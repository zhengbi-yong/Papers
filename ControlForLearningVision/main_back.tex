%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Zhengbi Yong}

\begin{document}

\twocolumn[
\icmltitle{Submission and Formatting Instructions for \\
           International Conference on Machine Learning (ICML 2024) Zhengbi Yong}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dawei Shi}{equal,yyy}
\icmlauthor{Zhengbi Yong}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    With the advancements in parallel hardware computing and computer technology, artificial intelligence (AI) has entered a new era of rapid development. However, current AI lacks systematic theoretical guidance, with model design and optimization primarily relying on empirical methods and trial-and-error approaches. Control theory, as a mature discipline developed over decades, possesses comprehensive methodologies and systematic design principles. Applying the concepts of control theory to the design and optimization of AI models holds the promise of further advancing the field of artificial intelligence. This paper reviews the progress in the application of control theory within AI, focusing on research in optimization algorithms, state-space models, reinforcement learning, system identification, model predictive control, robustness and stability, and optimal control. Additionally, future research directions are discussed.
    \end{abstract}

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2024 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \textbf{Simultaneous Submission:} ICML will not accept any paper which,
% at the time of submission, is under review for another conference or
% has already been published. This policy also applies to papers that
% overlap substantially in technical content with conference papers
% under review or previously published. ICML submissions must not be
% submitted to other conferences and journals during ICML's review
% period.
% %Authors may submit to ICML substantially different versions of journal papers
% %that are currently under review by the journal, but not yet accepted
% %at the time of submission.
% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2024}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{41}^{st}$ International Conference on Machine Learning},
% Vienna, Austria, PMLR 235, 2024.
% Copyright 2024 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2024\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2024\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2024 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2024.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2024 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2024.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}

% \bibliography{example_paper}
% \bibliographystyle{icml2024}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % APPENDIX
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In recent years, artificial intelligence has achieved remarkable progress, particularly driven by deep learning, yielding breakthroughs in computer vision, natural language processing, and other areas. However, the design of current AI models often lacks systematic theoretical guidance, relying heavily on large amounts of data and computational resources, as well as the experience of researchers. This reliance results in insufficient interpretability, robustness, and stability of models, limiting the application of AI in safety-critical domains.

Control theory, as a mature discipline, has developed comprehensive theoretical frameworks and systematic design methodologies over decades. In control theory, analysis and synthesis are two core tasks: analysis aims to ascertain system properties such as stability through theoretical means, while synthesis involves designing new systems or improving system performance based on theoretical guidance. Introducing control theory concepts into AI not only aids in the in-depth understanding and analysis of AI model properties but also guides the design of new models, enhancing their performance and robustness.

In practice, many AI models implicitly incorporate control theory principles. For instance, the residual connections in ResNet embody the concept of negative feedback, addressing the issues of vanishing and exploding gradients by learning residuals instead of absolute values. DenseNet's dense connections reflect the idea of forward propagation, effectively reusing shallow features. The latest Mamba model utilizes multiple state-space models, modeling nonlinear time-varying systems as linear time-invariant systems. These successful cases demonstrate the immense potential of control theory in AI.

This paper surveys the applications of control theory in AI, categorizing and interlinking related research works to provide guidance and reference for future studies.

\section{Classification of Control Theory Applications in Artificial Intelligence}

\subsection{Optimization and Control Algorithms in Deep Learning}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Accelerated optimization in deep learning with a proportional-integral-derivative controller
    \item PDE Models for Deep Neural Networks: Learning Theory, Calculus of Variations and Optimal Control
    \item An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks
    \item Maximum Principle Based Algorithms for Deep Learning
    \item Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms
    \item Deep Residual Learning for Image Recognition
\end{itemize}

\textbf{Review:}

Optimization algorithms are a core component of deep learning, with efficient optimizers significantly enhancing model training speed and performance. Control theory offers a wealth of optimization tools and theoretical foundations that can be leveraged to design new optimization algorithms.

\begin{itemize}
    \item \textbf{[1]} proposes an accelerated optimizer based on Proportional-Integral-Derivative (PID) controllers (PIDAO), integrating PID control concepts into the deep learning optimization process. Through the feedback mechanism of PID controllers, PIDAO accelerates convergence and improves model accuracy.
    
    \item \textbf{[2]} models deep neural networks as partial differential equations (PDEs) and formulates the learning task as an optimization problem constrained by PDEs. By introducing variational analysis and optimal control theory, the authors provide a new mathematical foundation for deep learning.
    
    \item \textbf{[36]} and \textbf{[37]} frame deep learning as a discrete-time optimal control problem, employing Pontryagin's Maximum Principle (PMP) to design new training algorithms that avoid pitfalls encountered by gradient-based methods, such as saddle point issues.
    
    \item \textbf{[38]} develops the Stochastic Modified Equations (SME) approach, approximating stochastic gradient algorithms as continuous-time stochastic differential equations and utilizing optimal control theory to derive new adaptive hyperparameter adjustment strategies.
    
    \item \textbf{[39]} introduces ResNet, which uses residual connections to facilitate the training of deeper networks. The underlying concept can be viewed as a form of control mechanism that helps mitigate the degradation problem in deep networks.
\end{itemize}

\textbf{Summary:}

Incorporating optimization concepts from control theory into deep learning can lead to the design of more efficient and stable optimization algorithms, enhancing the training speed and performance of models. This integration provides new perspectives and tools for optimization in deep learning.

\subsection{State-Space Models and Their Applications in Artificial Intelligence}
\textbf{Relevant Literature:}
\begin{itemize}
    \item State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era
    \item Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
    \item Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba
    \item Mamba: Linear-Time Sequence Modeling with Selective State Spaces
    \item On Recurrent Neural Networks for Learning-Based Control: Recent Results and Ideas for Future Developments
    \item Deep Learning via Dynamical Systems: An Approximation Perspective
    \item MAMKO: Mamba-Based Koopman Operator for Modeling and Predictive Control
\end{itemize}

\textbf{Review:}

State-space models (SSMs) are crucial tools in control theory, capable of describing the evolution of dynamic systems' states. Recently, SSMs have shown tremendous potential in handling long-sequence data, emerging as a powerful alternative to Transformer models.

\begin{itemize}
    \item \textbf{[6]} surveys state-space modeling in long sequence processing, exploring the resurgence of recurrent neural networks and the application of SSMs in sequence modeling.
    
    \item \textbf{[7]} links Transformers with SSMs, proposing a theoretical connection between SSMs and attention mechanisms, and designing the Mamba-2 architecture to enhance language modeling efficiency and performance.
    
    \item \textbf{[8]} and \textbf{[9]} build upon the Mamba framework to propose new models for 3D hand reconstruction and sequence modeling, leveraging the strengths of SSMs to achieve performance improvements across different tasks.
    
    \item \textbf{[28]} and \textbf{[29]} study deep learning models from the perspective of dynamical systems, exploring the application of recurrent neural networks in control design and proposing training frameworks based on Input-to-State Stability (ISS) and $\delta$ISS to enhance model robustness and verifiability.
    
    \item \textbf{[40]} combines Mamba with Koopman operators to introduce the MamKO framework for modeling and predictive control of nonlinear systems, demonstrating the application potential of SSMs in the control domain.
\end{itemize}

\textbf{Summary:}

SSMs provide new approaches for handling long-sequence data. By integrating state-space models from control theory into AI, more efficient and robust sequence models can be designed, which are significant for natural language processing, computer vision, and other fields.

\subsection{Reinforcement Learning and Control Theory}
\textbf{Relevant Literature:}
\begin{itemize}
    \item A General Control-Theoretic Approach for Reinforcement Learning: Theory and Algorithms
    \item Real-Time Progressive Learning: Accumulate Knowledge from Control with Neural-Network-Based Selective Memory
    \item Interpolation, Approximation and Controllability of Deep Neural Networks
\end{itemize}

\textbf{Review:}

Reinforcement learning (RL) aims to learn optimal policies through interaction with the environment, and control theory provides the theoretical foundations and tools for RL.

\begin{itemize}
    \item \textbf{[5]} designs a control-theoretic reinforcement learning method that directly learns optimal policies, establishing the convergence and optimality of control theoretic operators and introducing new policy gradient ascent theorems.
    
    \item \textbf{[15]} proposes the Real-Time Progressive Learning (RTPL) method, which utilizes neural networks with selective memory-based control schemes to continuously accumulate knowledge, enhancing learning efficiency and stability.
    
    \item \textbf{[25]} investigates the controllability and observability of deep neural networks from a control theory perspective, exploring the relationship between universal interpolation and universal approximation, thereby providing new insights for theoretical analysis of deep learning models.
\end{itemize}

\textbf{Summary:}

Integrating control theory into reinforcement learning not only offers theoretical guidance but also facilitates the design of new algorithms and strategies, improving learning efficiency and policy stability. This integration is crucial for decision-making and control in complex environments.

\subsection{System Identification and Modeling}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Physics-Constrained Taylor Neural Networks for Learning and Control of Dynamical Systems
    \item On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills
    \item Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems
    \item Towards Lifelong Learning of Recurrent Neural Networks for Control Design
\end{itemize}

\textbf{Review:}

System identification is a fundamental problem in control theory, focusing on learning dynamic models from data.

\begin{itemize}
    \item \textbf{[3]} introduces Monotonic Taylor Neural Networks (MTNN), integrating physical constraints into neural networks to ensure system monotonicity, thereby enhancing model generalization and robustness.
    
    \item \textbf{[21]} explores the application of Koopman operator theory in learning dexterous manipulation skills, utilizing simple yet powerful control theory structures to provide efficient policy learning methods.
    
    \item \textbf{[23]} combines tensor decomposition with control theory, proposing new methods for learning mixtures of linear dynamical systems, offering novel tools for system identification.
    
    \item \textbf{[26]} presents a lifelong learning approach for recurrent neural networks used in control system synthesis, addressing issues such as catastrophic forgetting and capacity saturation.
\end{itemize}

\textbf{Summary:}

Combining system identification methods from control theory with machine learning effectively learns complex system dynamics models, enhancing model interpretability and generalization. This synergy is significant for industrial control, robotics, and other domains.

\subsection{Model Predictive Control (MPC) and Artificial Intelligence}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Unifying Back-Propagation and Forward-Forward Algorithms Through Model Predictive Control
    \item A Cloud-Edge Framework for Energy-Efficient Event-Driven Control: An Integration of Online Supervised Learning, Spiking Neural Networks and Local Plasticity Rules
    \item PIDformer: Transformer Meets Control Theory
\end{itemize}

\textbf{Review:}

Model Predictive Control (MPC) is a model-based optimization control method that plays a crucial role in real-time decision-making.

\begin{itemize}
    \item \textbf{[4]} integrates MPC into the training of deep neural networks, unifying back-propagation (BP) and forward-forward (FF) algorithms to create a series of intermediate training algorithms with varying forecast horizons.
    
    \item \textbf{[11]} proposes a cloud-edge framework that leverages Spiking Neural Networks (SNNs) and local plasticity rules to achieve energy-efficient event-driven control, reducing communication demands between cloud and devices.
    
    \item \textbf{[41]} introduces PIDformer, which incorporates PID control mechanisms into Transformer architectures, enhancing model robustness and representational capacity while addressing inherent issues of attention mechanisms.
\end{itemize}

\textbf{Summary:}

Integrating MPC with AI models enables real-time, efficient decision-making and control, improving model robustness and stability. This combination is critical for applications requiring high real-time performance, such as autonomous driving and intelligent robotics.

\subsection{Enhancing Robustness and Stability}
\textbf{Relevant Literature:}
\begin{itemize}
    \item PID Control-Based Self-Healing to Improve the Robustness of Large Language Models
    \item PIDformer: Transformer Meets Control Theory
    \item Self-Healing Robust Neural Networks via Closed-Loop Control
    \item Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective
\end{itemize}

\textbf{Review:}

The robustness and stability of AI models are critical for their practical applications. Control theory provides effective tools to address these issues.

\begin{itemize}
    \item \textbf{[12]} proposes a self-healing process based on PID control to correct performance degradation of large language models under input disturbances, enhancing model robustness.
    
    \item \textbf{[13]} incorporates closed-loop feedback control systems into Transformer models through PIDformer, improving model stability and noise resistance.
    
    \item \textbf{[31]} addresses neural network robustness from a dynamical systems perspective by employing closed-loop control methods, proposing a self-healing neural network framework.
    
    \item \textbf{[33]} reviews deep learning theory from the perspectives of optimal control and dynamical systems, emphasizing the importance of dynamics and optimal control in developing deep learning theories.
\end{itemize}

\textbf{Summary:}

Applying robustness control and stability analysis methods from control theory can enhance the performance and reliability of AI models in complex environments, increasing their safety and applicability.

\subsection{Learning from an Optimal Control Perspective}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations
    \item A Mean-Field Optimal Control Formulation of Deep Learning
    \item An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks
\end{itemize}

\textbf{Review:}

Viewing deep learning through the lens of optimal control provides new theoretical foundations and algorithm design ideas for model training.

\begin{itemize}
    \item \textbf{[34]} develops the Stochastic Modified Equations (SME) framework to analyze the dynamics of stochastic gradient algorithms, revealing the convergence and generalization properties of these algorithms.
    
    \item \textbf{[35]} formulates the population risk minimization problem in deep learning as a mean-field optimal control problem, establishing Hamilton-Jacobi-Bellman and Pontryagin-type optimality conditions.
    
    \item \textbf{[36]} frames deep learning as a discrete-time optimal control problem, introducing a successive approximation method (MSA) based on Pontryagin's Maximum Principle for training neural networks, accompanied by rigorous error estimates.
\end{itemize}

\textbf{Summary:}

Optimal control theory offers systematic methods and tools for the training and analysis of deep learning models, enabling the design of more efficient and robust training algorithms and providing deeper insights into model behavior and characteristics.

\subsection{Viewing Deep Learning as Dynamical Systems}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Transfer Learning-Based Physics-Informed Convolutional Neural Network for Simulating Flow in Porous Media with Time-Varying Controls
    \item Direct Learning for Parameter-Varying Feedforward Control: A Neural-Network Approach
    \item Interpolation, Approximation and Controllability of Deep Neural Networks
    \item Forward and Inverse Approximation Theory for Linear Temporal Convolutional Networks
    \item Self-Healing Robust Neural Networks via Closed-Loop Control
    \item Deep Learning via Dynamical Systems: An Approximation Perspective
    \item Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective
\end{itemize}

\textbf{Review:}

Viewing deep learning models as dynamical systems allows the utilization of dynamical systems theory and methods for model analysis and design.

\begin{itemize}
    \item \textbf{[17]} introduces Physics-Informed Convolutional Neural Networks (PICNN) to simulate two-phase flow in porous media with time-varying well controls, applying dynamical systems concepts to neural network design.
    
    \item \textbf{[18]} proposes a direct learning method for parameter-varying feedforward control using neural networks, leveraging neural networks to learn dependencies of coefficients on scheduling signals.
    
    \item \textbf{[19]} investigates the expressive power of deep residual neural networks from a control theory perspective, exploring sufficient conditions for universal interpolation and approximation.
    
    \item \textbf{[24]} conducts theoretical analysis on the approximation properties of temporal convolutional networks, establishing approximation rate estimates and inverse approximation theorems.
    
    \item \textbf{[29]} studies the approximation capabilities of deep learning through the lens of dynamical systems flowcharts, establishing sufficient conditions for universal approximation and providing new approximation theories for deep learning.
    
    \item \textbf{[33]} reviews deep learning from the perspectives of dynamical systems and optimal control, emphasizing the significance of dynamics and optimal control in developing deep learning theories.
\end{itemize}

\textbf{Summary:}

Viewing deep learning models as dynamical systems enables the application of dynamical systems theory tools to gain deeper insights into model dynamics, analyze stability and convergence, and provide new ideas for model design and optimization.

\subsection{Applications of Control Theory in Neural Network Training and Architecture}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Reconciling Deep Learning and Control Theory: Recurrent Neural Networks for Indirect Data-Driven Control
    \item Asymptotically Fair Participation in Machine Learning Models: An Optimal Control Perspective
    \item On Recurrent Neural Networks for Learning-Based Control: Recent Results and Ideas for Future Developments
    \item Optimization in Machine Learning: A Distribution Space Approach
\end{itemize}

\textbf{Review:}

Control theory can provide theoretical guidance for neural network training and architecture design, enhancing model performance and generalization.

\begin{itemize}
    \item \textbf{[14]} discusses the potential of Recurrent Neural Networks (RNNs) in indirect data-driven control, designing frameworks for learning safe and robust RNN models.
    
    \item \textbf{[16]} proposes an optimal control problem for achieving asymptotically fair participation, using Pontryagin's Maximum Principle to design optimal control solutions that enhance long-term model performance.
    
    \item \textbf{[28]} explores the potential of RNNs in control design applications, investigating RNN training methods with ISS and $\delta$ISS guarantees to improve model robustness and verifiability.
    
    \item \textbf{[32]} posits that optimization problems in machine learning can be interpreted as convex optimization problems in function spaces, using appropriate relaxations to transform problems into convex optimization in distribution spaces, thereby providing new optimization methods.
\end{itemize}

\textbf{Summary:}

Control theory offers systematic methods for neural network training algorithms and architecture design, enhancing model robustness, interpretability, and performance. This provides new directions for the development of deep learning.

\subsection{Integration of Machine Learning and Control Theory}
\textbf{Relevant Literature:}
\begin{itemize}
    \item Machine Learning and Control Theory
    \item An Optimal Control View of LoRA and Binary Controller Design for Vision Transformers
    \item Parameter-Efficient Fine-Tuning with Controls
\end{itemize}

\textbf{Review:}

The integration of machine learning and control theory opens new possibilities for solving complex engineering problems.

\begin{itemize}
    \item \textbf{[30]} explores the connections between machine learning and control theory, reviewing the relationships between reinforcement learning, supervised learning, and control problems, and emphasizing the importance of control theory in machine learning.
    
    \item \textbf{[42]} examines low-rank adaptation (LoRA) and binary controller design for Vision Transformers from an optimal control perspective, proposing optimal control explorations based on Pontryagin's Maximum Principle.
    
    \item \textbf{[43]} introduces a parameter-efficient fine-tuning method based on control, treating control modules as control variables that perturb pre-trained models, thereby enhancing model adaptability and performance.
\end{itemize}

\textbf{Summary:}

Combining machine learning with control theory provides new tools and methods for modeling, optimization, and control of complex systems, fostering interdisciplinary research and applications.

\section{Fusion of Control Theory and Artificial Intelligence: Prospects and Challenges}

\subsection{Control Theory Providing Systematic Theoretical Guidance for AI}
Control theory possesses rigorous mathematical foundations and systematic design methodologies. Applying its principles to AI can offer theoretical guidance for model design, analysis, and optimization, thereby enhancing model interpretability and robustness.

\subsection{Enhancing Robustness and Stability of AI Models}
Utilizing robust control and stability analysis methods from control theory can improve the performance of AI models in complex environments, enhancing model safety and applicability. This addresses current vulnerabilities of models when facing disturbances and uncertainties.

\subsection{Deepening Understanding of AI Models}
By viewing deep learning models as dynamical or control systems, it is possible to gain a deeper understanding of their dynamic behaviors and characteristics, providing new ideas for model improvement and innovation.

\subsection{Future Research Directions}
\begin{itemize}
    \item \textbf{Development of New Optimization Algorithms:} Design more efficient and robust optimization algorithms based on control theory to enhance model training efficiency and performance.
    
    \item \textbf{Design of New Model Architectures:} Apply control theory concepts to model architecture design, developing models with better performance and robustness.
    
    \item \textbf{Enhancement of Model Interpretability:} Utilize control theory's analytical tools to gain deeper insights into model mechanisms, improving model interpretability.
    
    \item \textbf{Interdisciplinary Research:} Encourage collaboration between researchers in AI and control theory to foster interdisciplinary innovation and applications.
\end{itemize}

\section{Conclusion}
The application of control theory in artificial intelligence provides new perspectives and tools for model design, optimization, and analysis. By integrating control theory concepts into AI, it is possible to enhance model performance, robustness, and interpretability, offering potential solutions to current challenges in the AI field. In the future, as research progresses, the fusion of control theory and artificial intelligence will lay a solid theoretical foundation for the safe and efficient operation of intelligent systems, driving the development and innovation of the AI domain.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
